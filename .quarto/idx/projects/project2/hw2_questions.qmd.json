{"title":"Poisson Regression Examples","markdown":{"yaml":{"title":"Poisson Regression Examples","author":"Mrunmayee Inamke","date":"today","callout-appearance":"minimal","format":{"html":{"code-fold":true,"math":true}}},"headingText":"Blueprinty Case Study","containsRefs":false,"markdown":"\n\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n```\n::::\n\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, multiple=\"dodge\")\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n```\n::::\n\n\n### Means of Number of Patents by Customer Status\n:::: {.callout-note collapse=\"true\"}\n```{python}\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n```\n::::\n\n### Observation\n\nFirms that are customers of Blueprinty tend to have more patents on average (≈ 4.13) than non-customers (≈ 3.47). The histogram shows that customer firms are more concentrated at higher patent counts, suggesting a potential positive relationship between using Blueprinty and patent success. However, this pattern may also be influenced by other factors, such as region or firm age, which need to be explored further.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n### Compare Regions and Ages by Customer Status\n\n### Region Distribution by Customer Status\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npalette = {0: \"#4C72B0\", 1: \"#DD8452\"}\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\", palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n```\n::::\n\n### Firm Age by Customer Status\n:::: {.callout-note collapse=\"true\"}\n```{python}\n# Map customer status labels\ndf[\"Customer Status\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Define matching palette\npalette = {\"Non-Customer\": \"#4C72B0\", \"Customer\": \"#DD8452\"}\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x=\"Customer Status\", y=\"age\", hue=\"Customer Status\", palette=palette, legend=False)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.tight_layout()\nplt.show()\n```\n::::\n\n### Mean of ages by Customer Status\n:::: {.callout-note collapse=\"true\"}\n```{python}\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n```\n::::\n\n### Observation\nThe regional distribution of firms is not uniform between customers and non-customers. Some regions (e.g., Northeast) have a higher concentration of Blueprinty customers. This suggests that region may confound the relationship between software usage and patent outcomes.\n\nRegarding age, customers are slightly older on average (~26.9 years) than non-customers (~26.1 years), though the difference is modest. It is still important to consider firm age in the analysis to avoid biased conclusions.\n\n### Estimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n### Likelihood for Poisson Distribution\n\n**Probability Mass Function**\n\nThe probability mass function for a single observation from a Poisson distribution is:\n\n$$\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\n**Likelihood Function**\n\nAssuming the observations are independent, the likelihood function for the entire dataset is:\n\n$$\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nThis can be rewritten as:\n\n$$\nL(\\lambda) = e^{-n\\lambda} \\cdot \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n$$\n\n**Log-Likelihood Function**\n\nTaking the natural logarithm of the likelihood gives:\n\n$$\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n$$\n\nThis log-likelihood will be used to estimate \\( \\lambda \\) via Maximum Likelihood Estimation (MLE).\n\n### Poisson Log Likelihood Function\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd <= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n```\n::::\n\nWe visualize the Poisson log-likelihood as a function of \\( \\lambda \\), where the maximum corresponds to the MLE.\n\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n::::\n\n### Derivation of the MLE for Poisson Distribution\n\nSuppose \\( Y_1, Y_2, \\ldots, Y_n \\sim \\text{Poisson}(\\lambda) \\), where the probability mass function is:\n\n$$\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\n**Step 1: Log-Likelihood Function**\n\nThe log-likelihood of the entire sample is:\n\n$$\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n$$\n\nWe can simplify this (since \\( \\log Y_i! \\) does not depend on \\( \\lambda \\)):\n\n$$\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n$$\n\n**Step 2: First Derivative**\n\nTake the derivative with respect to \\( \\lambda \\):\n\n$$\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n$$\n\n**Step 3: Set Derivative to Zero**\n\n$$\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n$$\n\nSolving for \\( \\lambda \\):\n\n$$\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n$$\n\nThe maximum likelihood estimator (MLE) of \\( \\lambda \\) is the sample mean:\n\n$$\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n$$\n\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to \\( \\lambda \\), so the best estimate of \\( \\lambda \\) from data is the observed average.\n\n### Maximum Likelihood Estimation using `scipy.optimize`\n\nWe use numerical optimization to find the value of \\( \\lambda \\) that maximizes the Poisson log-likelihood.\n\n:::: {.callout-note collapse=\"true\"}\n```{python}\nfrom scipy import optimize\nneg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)\nresult = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n```\n::::\n\n### Interpretation\n\nThe maximum likelihood estimate (MLE) of \\( \\lambda \\) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.\n\n```{python}\n# Compare with the sample mean\ndf[\"patents\"].mean()\n```\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n::::{.callout-note collapse=\"true\"}\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson regression log-likelihood function\ndef poisson_regression_loglike(beta, X, Y):\n    Xbeta = X @ beta\n    lambdas = np.exp(Xbeta)\n    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))\n```\n::::\n\n### To find the MLE vector using `scipy.optimize`\n\n::::{.callout-note collapse=\"true\"}\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create age squared\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create region dummies (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[\"age\"],\n    df[\"age2\"],\n    region_dummies,\n    df[\"iscustomer\"]\n], axis=1)\n\nY = df[\"patents\"].values\nX_matrix = X.values\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent\n    lam = np.exp(Xb_clipped)\n\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\nsummary\n```\n::::\n\n### Validate Results Using `statsmodels.GLM()`\n\nTo confirm the accuracy of our manual MLE implementation, we use `statsmodels.GLM()` to estimate the same Poisson regression model:\n\n::::{.callout-note collapse=\"true\"}\n```{python}\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n### Coefficients and Standard Errors from Poisson Regression\n# Extract coefficient summary\ncoef_table = glm_results.summary2().tables[1][[\"Coef.\", \"Std.Err.\"]]\ncoef_table.rename(columns={\"Coef.\": \"Coefficient\", \"Std.Err.\": \"Std. Error\"}, inplace=True)\n\n# Display table\ncoef_table\n```\n::::\n\n### Interpretation \n- **Age** has a strong positive effect on patent activity: older firms are more likely to have more patents.\n- **Age²** is negative and significant, suggesting a diminishing return — patent output increases with age but at a decreasing rate.\n- **Blueprinty’s software** has a statistically significant positive coefficient of 0.2076 (p < 0.001). This implies firms using the software are expected to have **23% more patents** than comparable non-customers.\n- **Regional effects** (Northeast, Northwest, etc.) are not statistically significant, suggesting location does not materially affect patent outcomes once other factors are controlled for.\n\nEstimate Effect of Blueprinty's Software via **Counterfactual Prediction**\n\nWe simulate two scenarios:\n\n- `X_0`: All firms set to **non-customer** (`iscustomer = 0`)\n- `X_1`: All firms set to **customer** (`iscustomer = 1`)\n\nThen we compare the predicted number of patents for each firm under the two scenarios using the fitted model.\n\n```{python}\n# Make two versions of X_glm:\n# X_0: all firms are non-customers\n# X_1: all firms are customers\nX_0 = X_glm.copy()\nX_1 = X_glm.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Estimate average treatment effect\naverage_effect = np.mean(y_pred_1 - y_pred_0)\n```\n\n### Interpretation\nThe average difference in predicted number of patents between Blueprinty customers and non-customers is:\n```{python}\nprint(f\"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}\")\n```\n\nThis quantifies the effect of Blueprinty's software: firms using it are predicted to file approximately **0.793** more patents over 5 years, on average, than similar firms who don’t use it, controlling for age and region.\n\n## AirBnB Case Study\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n::::\n\n### Data Cleaning\n\nWe begin by dropping listings with missing values in relevant variables, then perform basic EDA on the cleaned dataset.\n\n### Drop Rows with Missing Data\n\n```{python}\nimport pandas as pd\ndf = pd.read_csv(\"airbnb.csv\")\nrelevant_cols = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[relevant_cols].dropna()\n```\n### Exploratory Data Analysis (EDA)\n\n### Distribution of Number of Reviews\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.histplot(\n    df_clean[\"number_of_reviews\"],\n    bins=50,\n    kde=False,\n    color=\"#1F78B4\",\n\n)\n\nplt.title(\"Distribution of Number of Reviews\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Number of Reviews\", fontsize=12)\nplt.ylabel(\"Count of Listings\", fontsize=12)\nplt.xlim(0, 100)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n```\n\n### Average Number of Reviews by Room Type \n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Data\navg_reviews = df_clean.groupby(\"room_type\")[\"number_of_reviews\"].mean().reset_index()\n\n# Better-looking custom blue palette\ncustom_blue_palette = [\"#A6CEE3\", \"#1F78B4\", \"#08519C\"]\n\n# Plot with warning suppression\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=FutureWarning)\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews,\n        x=\"room_type\",\n        y=\"number_of_reviews\",\n        palette=custom_blue_palette,\n    )\n\nplt.title(\"Average Number of Reviews by Room Type\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Room Type\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n```\n\n### Average Number of Reviews by Instant Bookability\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\navg_reviews_by_bookable = df_clean.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean().reset_index()\navg_reviews_by_bookable[\"instant_bookable\"] = avg_reviews_by_bookable[\"instant_bookable\"].map({\"f\": \"No\", \"t\": \"Yes\"})\n\nblue_palette = {\"No\": \"#6baed6\", \"Yes\": \"#2171b5\"}\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews_by_bookable,\n        x=\"instant_bookable\",\n        y=\"number_of_reviews\",\n        hue=\"instant_bookable\", \n        palette=blue_palette,\n        legend=False,\n    )\nplt.title(\"Average Number of Reviews by Instant Bookability\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Instant Bookable\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.ylim(0, avg_reviews_by_bookable[\"number_of_reviews\"].max() * 1.1)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n```\n\n### Correlation with Numeric Predictors\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnumeric_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\ncorrelation_matrix = df_clean[numeric_vars].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    correlation_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"Blues\",             \n    vmin=0, vmax=1,          \n    square=True,\n    linewidths=0.75,\n    linecolor=\"white\",\n    annot_kws={\"fontsize\": 10, \"weight\": \"bold\"}\n)\nplt.title(\"Correlation Matrix of Numeric Variables\", fontsize=14, weight=\"bold\")\nplt.xticks(rotation=45, ha=\"right\", fontsize=10)\nplt.yticks(rotation=0, fontsize=10)\nplt.tight_layout()\nplt.show()\n```\n\n### Poisson Regression Model Using `statsmodels.GLM()`\n\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors:\n- `room_type` (categorical)\n- `instant_bookable` (binary)\n- `price`, `days`, `bathrooms`, `bedrooms`\n- Review scores: `cleanliness`, `location`, `value`\n\n```{python}\nimport statsmodels.api as sm\n\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Create design matrix\nX = pd.concat([\n    df_clean[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n              \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n              \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\nX = sm.add_constant(X)\nX = X.astype(float)  \n\nY = df_clean[\"number_of_reviews\"]\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nsummary_df = poisson_results.summary2().tables[1]\n\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"P>|z|\": \"P-Value\"\n})\n\nsignificant_results = summary_df[summary_df[\"P-Value\"] < 0.05][[\"Coefficient\", \"Std. Error\", \"P-Value\"]]\n\nsignificant_results = significant_results.round(4)\n\nsignificant_results\n```\n\n### Interpretation\n- **Intercept (`3.4980`)**  \n  The baseline log-expected number of reviews for a listing when all other variables are zero (serves as a reference point).\n\n- **Price (`-0.0000`)**  \n  As price increases, the expected number of reviews decreases slightly. This effect is small but statistically significant, indicating higher-priced listings may deter some bookings.\n\n- **Days Active (`+0.0001`)**  \n  Listings that have been active longer tend to accumulate more reviews. This reflects more exposure over time.\n\n- **Bathrooms (`-0.1177`)**  \n  Surprisingly, listings with more bathrooms tend to receive fewer reviews. This might reflect that larger or luxury properties are booked less frequently.\n\n- **Bedrooms (`+0.0741`)**  \n  Listings with more bedrooms attract more bookings, likely due to their ability to accommodate larger groups.\n\n- **Cleanliness Score (`+0.1131`)**  \n  Clean listings receive more reviews, reinforcing the importance of cleanliness to guests.\n\n- **Location Score (`-0.0769`)**  \n  A negative association with reviews, possibly due to limited variation in location ratings or multicollinearity with other variables.\n\n- **Value Score (`-0.0911`)**  \n  Higher value scores are associated with fewer reviews, which may reflect that guests leave high value ratings in less competitive or less popular markets.\n\n- **Instant Bookable (`+0.3459`)**  \n  Listings that support instant booking are expected to receive approximately **41% more reviews** than those that do not:  \n  \\[\n  \\exp(0.3459) \\approx 1.41\n  \\]\n- **Private Room (`-0.0105`)**  \n  Private rooms receive slightly fewer reviews than entire homes, likely due to lower demand or guest preferences for full space.\n\n- **Shared Room (`-0.2463`)**  \n  Shared rooms receive significantly fewer reviews — about **22% fewer** than entire homes:  \n  \\[\n  \\exp(-0.2463) \\approx 0.78\n  \\]\n\n","srcMarkdownNoYaml":"\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n```\n::::\n\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, multiple=\"dodge\")\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n```\n::::\n\n\n### Means of Number of Patents by Customer Status\n:::: {.callout-note collapse=\"true\"}\n```{python}\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n```\n::::\n\n### Observation\n\nFirms that are customers of Blueprinty tend to have more patents on average (≈ 4.13) than non-customers (≈ 3.47). The histogram shows that customer firms are more concentrated at higher patent counts, suggesting a potential positive relationship between using Blueprinty and patent success. However, this pattern may also be influenced by other factors, such as region or firm age, which need to be explored further.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n### Compare Regions and Ages by Customer Status\n\n### Region Distribution by Customer Status\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npalette = {0: \"#4C72B0\", 1: \"#DD8452\"}\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\", palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n```\n::::\n\n### Firm Age by Customer Status\n:::: {.callout-note collapse=\"true\"}\n```{python}\n# Map customer status labels\ndf[\"Customer Status\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Define matching palette\npalette = {\"Non-Customer\": \"#4C72B0\", \"Customer\": \"#DD8452\"}\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x=\"Customer Status\", y=\"age\", hue=\"Customer Status\", palette=palette, legend=False)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.tight_layout()\nplt.show()\n```\n::::\n\n### Mean of ages by Customer Status\n:::: {.callout-note collapse=\"true\"}\n```{python}\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n```\n::::\n\n### Observation\nThe regional distribution of firms is not uniform between customers and non-customers. Some regions (e.g., Northeast) have a higher concentration of Blueprinty customers. This suggests that region may confound the relationship between software usage and patent outcomes.\n\nRegarding age, customers are slightly older on average (~26.9 years) than non-customers (~26.1 years), though the difference is modest. It is still important to consider firm age in the analysis to avoid biased conclusions.\n\n### Estimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n### Likelihood for Poisson Distribution\n\n**Probability Mass Function**\n\nThe probability mass function for a single observation from a Poisson distribution is:\n\n$$\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\n**Likelihood Function**\n\nAssuming the observations are independent, the likelihood function for the entire dataset is:\n\n$$\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nThis can be rewritten as:\n\n$$\nL(\\lambda) = e^{-n\\lambda} \\cdot \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n$$\n\n**Log-Likelihood Function**\n\nTaking the natural logarithm of the likelihood gives:\n\n$$\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n$$\n\nThis log-likelihood will be used to estimate \\( \\lambda \\) via Maximum Likelihood Estimation (MLE).\n\n### Poisson Log Likelihood Function\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd <= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n```\n::::\n\nWe visualize the Poisson log-likelihood as a function of \\( \\lambda \\), where the maximum corresponds to the MLE.\n\n:::: {.callout-note collapse=\"true\"}\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n::::\n\n### Derivation of the MLE for Poisson Distribution\n\nSuppose \\( Y_1, Y_2, \\ldots, Y_n \\sim \\text{Poisson}(\\lambda) \\), where the probability mass function is:\n\n$$\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\n**Step 1: Log-Likelihood Function**\n\nThe log-likelihood of the entire sample is:\n\n$$\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n$$\n\nWe can simplify this (since \\( \\log Y_i! \\) does not depend on \\( \\lambda \\)):\n\n$$\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n$$\n\n**Step 2: First Derivative**\n\nTake the derivative with respect to \\( \\lambda \\):\n\n$$\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n$$\n\n**Step 3: Set Derivative to Zero**\n\n$$\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n$$\n\nSolving for \\( \\lambda \\):\n\n$$\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n$$\n\nThe maximum likelihood estimator (MLE) of \\( \\lambda \\) is the sample mean:\n\n$$\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n$$\n\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to \\( \\lambda \\), so the best estimate of \\( \\lambda \\) from data is the observed average.\n\n### Maximum Likelihood Estimation using `scipy.optimize`\n\nWe use numerical optimization to find the value of \\( \\lambda \\) that maximizes the Poisson log-likelihood.\n\n:::: {.callout-note collapse=\"true\"}\n```{python}\nfrom scipy import optimize\nneg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)\nresult = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n```\n::::\n\n### Interpretation\n\nThe maximum likelihood estimate (MLE) of \\( \\lambda \\) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.\n\n```{python}\n# Compare with the sample mean\ndf[\"patents\"].mean()\n```\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n::::{.callout-note collapse=\"true\"}\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson regression log-likelihood function\ndef poisson_regression_loglike(beta, X, Y):\n    Xbeta = X @ beta\n    lambdas = np.exp(Xbeta)\n    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))\n```\n::::\n\n### To find the MLE vector using `scipy.optimize`\n\n::::{.callout-note collapse=\"true\"}\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create age squared\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create region dummies (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[\"age\"],\n    df[\"age2\"],\n    region_dummies,\n    df[\"iscustomer\"]\n], axis=1)\n\nY = df[\"patents\"].values\nX_matrix = X.values\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent\n    lam = np.exp(Xb_clipped)\n\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\nsummary\n```\n::::\n\n### Validate Results Using `statsmodels.GLM()`\n\nTo confirm the accuracy of our manual MLE implementation, we use `statsmodels.GLM()` to estimate the same Poisson regression model:\n\n::::{.callout-note collapse=\"true\"}\n```{python}\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n### Coefficients and Standard Errors from Poisson Regression\n# Extract coefficient summary\ncoef_table = glm_results.summary2().tables[1][[\"Coef.\", \"Std.Err.\"]]\ncoef_table.rename(columns={\"Coef.\": \"Coefficient\", \"Std.Err.\": \"Std. Error\"}, inplace=True)\n\n# Display table\ncoef_table\n```\n::::\n\n### Interpretation \n- **Age** has a strong positive effect on patent activity: older firms are more likely to have more patents.\n- **Age²** is negative and significant, suggesting a diminishing return — patent output increases with age but at a decreasing rate.\n- **Blueprinty’s software** has a statistically significant positive coefficient of 0.2076 (p < 0.001). This implies firms using the software are expected to have **23% more patents** than comparable non-customers.\n- **Regional effects** (Northeast, Northwest, etc.) are not statistically significant, suggesting location does not materially affect patent outcomes once other factors are controlled for.\n\nEstimate Effect of Blueprinty's Software via **Counterfactual Prediction**\n\nWe simulate two scenarios:\n\n- `X_0`: All firms set to **non-customer** (`iscustomer = 0`)\n- `X_1`: All firms set to **customer** (`iscustomer = 1`)\n\nThen we compare the predicted number of patents for each firm under the two scenarios using the fitted model.\n\n```{python}\n# Make two versions of X_glm:\n# X_0: all firms are non-customers\n# X_1: all firms are customers\nX_0 = X_glm.copy()\nX_1 = X_glm.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Estimate average treatment effect\naverage_effect = np.mean(y_pred_1 - y_pred_0)\n```\n\n### Interpretation\nThe average difference in predicted number of patents between Blueprinty customers and non-customers is:\n```{python}\nprint(f\"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}\")\n```\n\nThis quantifies the effect of Blueprinty's software: firms using it are predicted to file approximately **0.793** more patents over 5 years, on average, than similar firms who don’t use it, controlling for age and region.\n\n## AirBnB Case Study\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n::::\n\n### Data Cleaning\n\nWe begin by dropping listings with missing values in relevant variables, then perform basic EDA on the cleaned dataset.\n\n### Drop Rows with Missing Data\n\n```{python}\nimport pandas as pd\ndf = pd.read_csv(\"airbnb.csv\")\nrelevant_cols = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[relevant_cols].dropna()\n```\n### Exploratory Data Analysis (EDA)\n\n### Distribution of Number of Reviews\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.histplot(\n    df_clean[\"number_of_reviews\"],\n    bins=50,\n    kde=False,\n    color=\"#1F78B4\",\n\n)\n\nplt.title(\"Distribution of Number of Reviews\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Number of Reviews\", fontsize=12)\nplt.ylabel(\"Count of Listings\", fontsize=12)\nplt.xlim(0, 100)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n```\n\n### Average Number of Reviews by Room Type \n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Data\navg_reviews = df_clean.groupby(\"room_type\")[\"number_of_reviews\"].mean().reset_index()\n\n# Better-looking custom blue palette\ncustom_blue_palette = [\"#A6CEE3\", \"#1F78B4\", \"#08519C\"]\n\n# Plot with warning suppression\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=FutureWarning)\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews,\n        x=\"room_type\",\n        y=\"number_of_reviews\",\n        palette=custom_blue_palette,\n    )\n\nplt.title(\"Average Number of Reviews by Room Type\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Room Type\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n```\n\n### Average Number of Reviews by Instant Bookability\n```{python}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\navg_reviews_by_bookable = df_clean.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean().reset_index()\navg_reviews_by_bookable[\"instant_bookable\"] = avg_reviews_by_bookable[\"instant_bookable\"].map({\"f\": \"No\", \"t\": \"Yes\"})\n\nblue_palette = {\"No\": \"#6baed6\", \"Yes\": \"#2171b5\"}\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews_by_bookable,\n        x=\"instant_bookable\",\n        y=\"number_of_reviews\",\n        hue=\"instant_bookable\", \n        palette=blue_palette,\n        legend=False,\n    )\nplt.title(\"Average Number of Reviews by Instant Bookability\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Instant Bookable\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.ylim(0, avg_reviews_by_bookable[\"number_of_reviews\"].max() * 1.1)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n```\n\n### Correlation with Numeric Predictors\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnumeric_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\ncorrelation_matrix = df_clean[numeric_vars].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    correlation_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"Blues\",             \n    vmin=0, vmax=1,          \n    square=True,\n    linewidths=0.75,\n    linecolor=\"white\",\n    annot_kws={\"fontsize\": 10, \"weight\": \"bold\"}\n)\nplt.title(\"Correlation Matrix of Numeric Variables\", fontsize=14, weight=\"bold\")\nplt.xticks(rotation=45, ha=\"right\", fontsize=10)\nplt.yticks(rotation=0, fontsize=10)\nplt.tight_layout()\nplt.show()\n```\n\n### Poisson Regression Model Using `statsmodels.GLM()`\n\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors:\n- `room_type` (categorical)\n- `instant_bookable` (binary)\n- `price`, `days`, `bathrooms`, `bedrooms`\n- Review scores: `cleanliness`, `location`, `value`\n\n```{python}\nimport statsmodels.api as sm\n\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Create design matrix\nX = pd.concat([\n    df_clean[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n              \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n              \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\nX = sm.add_constant(X)\nX = X.astype(float)  \n\nY = df_clean[\"number_of_reviews\"]\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nsummary_df = poisson_results.summary2().tables[1]\n\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"P>|z|\": \"P-Value\"\n})\n\nsignificant_results = summary_df[summary_df[\"P-Value\"] < 0.05][[\"Coefficient\", \"Std. Error\", \"P-Value\"]]\n\nsignificant_results = significant_results.round(4)\n\nsignificant_results\n```\n\n### Interpretation\n- **Intercept (`3.4980`)**  \n  The baseline log-expected number of reviews for a listing when all other variables are zero (serves as a reference point).\n\n- **Price (`-0.0000`)**  \n  As price increases, the expected number of reviews decreases slightly. This effect is small but statistically significant, indicating higher-priced listings may deter some bookings.\n\n- **Days Active (`+0.0001`)**  \n  Listings that have been active longer tend to accumulate more reviews. This reflects more exposure over time.\n\n- **Bathrooms (`-0.1177`)**  \n  Surprisingly, listings with more bathrooms tend to receive fewer reviews. This might reflect that larger or luxury properties are booked less frequently.\n\n- **Bedrooms (`+0.0741`)**  \n  Listings with more bedrooms attract more bookings, likely due to their ability to accommodate larger groups.\n\n- **Cleanliness Score (`+0.1131`)**  \n  Clean listings receive more reviews, reinforcing the importance of cleanliness to guests.\n\n- **Location Score (`-0.0769`)**  \n  A negative association with reviews, possibly due to limited variation in location ratings or multicollinearity with other variables.\n\n- **Value Score (`-0.0911`)**  \n  Higher value scores are associated with fewer reviews, which may reflect that guests leave high value ratings in less competitive or less popular markets.\n\n- **Instant Bookable (`+0.3459`)**  \n  Listings that support instant booking are expected to receive approximately **41% more reviews** than those that do not:  \n  \\[\n  \\exp(0.3459) \\approx 1.41\n  \\]\n- **Private Room (`-0.0105`)**  \n  Private rooms receive slightly fewer reviews than entire homes, likely due to lower demand or guest preferences for full space.\n\n- **Shared Room (`-0.2463`)**  \n  Shared rooms receive significantly fewer reviews — about **22% fewer** than entire homes:  \n  \\[\n  \\exp(-0.2463) \\approx 0.78\n  \\]\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"hw2_questions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.5","theme":["cosmo","brand"],"title":"Poisson Regression Examples","author":"Mrunmayee Inamke","date":"today","callout-appearance":"minimal","math":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}