---
title: "Poisson Regression Examples"
author: "Mrunmayee Inamke"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
format:
  html:
    code-fold: true
    math: true
---

## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data
:::: {.callout-note collapse="true"}
```{python}
import pandas as pd
blue_df = pd.read_csv("blueprinty.csv")
blue_df.head()
```
::::

:::: {.callout-note collapse="true"}
```{python}
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))
sns.histplot(data=blue_df, x="patents", hue="iscustomer", bins=20, multiple="dodge")
plt.title("Number of Patents by Customer Status")
plt.xlabel("Number of Patents")
plt.ylabel("Number of Firms")
plt.legend(title="Is Customer", labels=["Non-Customer", "Customer"])
plt.show()
```
::::


### Means of Number of Patents by Customer Status
:::: {.callout-note collapse="true"}
```{python}
blue_df.groupby("iscustomer")["patents"].mean()
```
::::

### Observation

Firms that use Blueprinty hold more patents on average (~4.13) compared to non-customers (~3.47). The histogram shows a greater concentration of customer firms at higher patent counts, hinting at a possible link between Blueprinty use and innovation outcomes.

However, this pattern may reflect underlying differences — such as firm age or geographic region — rather than a direct effect of Blueprinty. Since customer firms aren't randomly assigned, it's important to control for these systematic differences in any causal analysis.

### Compare Regions and Ages by Customer Status

### Region Distribution by Customer Status
:::: {.callout-note collapse="true"}
```{python}
import seaborn as sns
import matplotlib.pyplot as plt

palette = {0: "#4C72B0", 1: "#DD8452"}
plt.figure(figsize=(8, 6))
sns.countplot(data=blue_df, x="region", hue="iscustomer", palette=palette)
plt.title("Region Distribution by Customer Status")
plt.xlabel("Region")
plt.ylabel("Number of Firms")
plt.legend(title="Is Customer", labels=["Non-Customer", "Customer"])
plt.tight_layout()
plt.show()
```
::::

### Firm Age by Customer Status
:::: {.callout-note collapse="true"}
```{python}
# Map customer status labels
blue_df["Customer Status"] = blue_df["iscustomer"].map({0: "Non-Customer", 1: "Customer"})

# Define matching palette
palette = {"Non-Customer": "#4C72B0", "Customer": "#DD8452"}

# Plot
plt.figure(figsize=(8, 6))
sns.boxplot(data=blue_df, x="Customer Status", y="age", hue="Customer Status", palette=palette, legend=False)
plt.title("Firm Age by Customer Status")
plt.xlabel("Is Customer")
plt.ylabel("Firm Age (Years)")
plt.tight_layout()
plt.show()
```
::::

### Mean of ages by Customer Status
:::: {.callout-note collapse="true"}
```{python}
blue_df.groupby("iscustomer")["age"].mean()
```
::::

### Observation
The geographic distribution of firms differs between Blueprinty customers and non-customers — with regions like the Northeast having a higher share of customers. This uneven distribution suggests that region may confound the observed link between Blueprinty use and patent outcomes.

In terms of firm age, customers are on average slightly older (~26.9 years) than non-customers (~26.1 years). While the difference is small, it's still important to account for age to ensure unbiased analysis.

### Estimation of Simple Poisson Model
Because our outcome — number of patents — is a count variable (small integers over a fixed time frame), we model it using a Poisson distribution. We begin by estimating a simple Poisson regression via Maximum Likelihood Estimation, capturing how Blueprinty usage relates to patent output over the past 5 years.


### Likelihood for Poisson Distribution

**Probability Mass Function**

The probability mass function for a single observation from a Poisson distribution is:

$$
f(Y_i \mid \lambda) = \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

**Likelihood Function**

Assuming the observations are independent, the likelihood function for the entire dataset is:

$$
L(\lambda; Y_1, \ldots, Y_n) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

This can be rewritten as:

$$
L(\lambda) = e^{-n\lambda} \cdot \lambda^{\sum_{i=1}^{n} Y_i} \cdot \prod_{i=1}^{n} \frac{1}{Y_i!}
$$

**Log-Likelihood Function**

Taking the natural logarithm of the likelihood gives:

$$
\log L(\lambda) = -n\lambda + \left( \sum_{i=1}^{n} Y_i \right) \log \lambda - \sum_{i=1}^{n} \log(Y_i!)
$$

This log-likelihood will be used to estimate \( \lambda \) via Maximum Likelihood Estimation (MLE).

### Poisson Log Likelihood Function
:::: {.callout-note collapse="true"}
```{python}
import numpy as np
from scipy.special import gammaln

# Define the Poisson log-likelihood function
def poisson_loglikelihood(lambd, Y):
    if lambd <= 0:
        return -np.inf  # log-likelihood is undefined for non-positive lambda
    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))
```
::::

We visualize the Poisson log-likelihood as a function of \( \lambda \), where the maximum corresponds to the MLE.

:::: {.callout-note collapse="true"}
```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import gammaln

# Example data: simulated Poisson observations
Y_sample = blue_df["patents"].values

# Range of lambda values to plot
lambdas = np.linspace(0.1, 10, 300)
log_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]

# Find MLE visually
lambda_mle = lambdas[np.argmax(log_likelihoods)]

# Plot
plt.figure(figsize=(8, 6))
plt.plot(lambdas, log_likelihoods, linewidth=2, color="steelblue")
plt.axvline(lambda_mle, color="darkorange", linestyle="--", label=f"MLE ≈ {lambda_mle:.2f}")
plt.title("Poisson Log-Likelihood vs. Lambda", fontsize=14, weight="bold")
plt.xlabel("Lambda", fontsize=12)
plt.ylabel("Log-Likelihood", fontsize=12)
plt.grid(True, linestyle="--", alpha=0.5)
plt.legend()
plt.tight_layout()
plt.show()
```
::::

### Derivation of the MLE for Poisson Distribution

Suppose \( Y_1, Y_2, \ldots, Y_n \sim \text{Poisson}(\lambda) \), where the probability mass function is:

$$
f(Y_i \mid \lambda) = \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

**Step 1: Log-Likelihood Function**

The log-likelihood of the entire sample is:

$$
\log L(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log Y_i! \right)
$$

We can simplify this (since \( \log Y_i! \) does not depend on \( \lambda \)):

$$
\log L(\lambda) = -n\lambda + \left( \sum_{i=1}^{n} Y_i \right) \log \lambda + \text{constant}
$$

**Step 2: First Derivative**

Take the derivative with respect to \( \lambda \):

$$
\frac{d}{d\lambda} \log L(\lambda) = -n + \frac{1}{\lambda} \sum_{i=1}^{n} Y_i
$$

**Step 3: Set Derivative to Zero**

$$
-n + \frac{1}{\lambda} \sum_{i=1}^{n} Y_i = 0
$$

Solving for \( \lambda \):

$$
\lambda = \frac{1}{n} \sum_{i=1}^{n} Y_i = \bar{Y}
$$

The maximum likelihood estimator (MLE) of \( \lambda \) is the sample mean:

$$
\hat{\lambda}_{\text{MLE}} = \bar{Y}
$$

This result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to \( \lambda \), so the best estimate of \( \lambda \) from data is the observed average.

### Maximum Likelihood Estimation using `scipy.optimize`

We use numerical optimization to find the value of \( \lambda \) that maximizes the Poisson log-likelihood.

:::: {.callout-note collapse="true"}
```{python}
from scipy import optimize
neg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)
result = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])
lambda_mle = result.x[0]
lambda_mle
```
::::

### Interpretation

The maximum likelihood estimate (MLE) of \( \lambda \) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.

```{python}
# Compare with the sample mean
blue_df["patents"].mean()
```

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

::::{.callout-note collapse="true"}
```{python}
import numpy as np
from scipy.special import gammaln

# Define Poisson regression log-likelihood function
def poisson_regression_loglike(beta, X, Y):
    Xbeta = X @ beta
    lambdas = np.exp(Xbeta)
    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))
```
::::

### To find the MLE vector using `scipy.optimize`

::::{.callout-note collapse="true"}
```{python}
import pandas as pd
import numpy as np
from scipy import optimize
from scipy.special import gammaln

# Use blue_df instead of df
# Create age squared
blue_df["age2"] = blue_df["age"] ** 2

# Create region dummies (drop one to avoid multicollinearity)
region_dummies = pd.get_dummies(blue_df["region"], drop_first=True)

# Construct design matrix
X = pd.concat([
  pd.Series(1, index=blue_df.index, name="intercept"),
  blue_df["age"],
  blue_df["age2"],
  region_dummies,
  blue_df["iscustomer"]
], axis=1)

Y = blue_df["patents"].values
X_matrix = X.values

def poisson_loglike(beta, X, Y):
  beta = np.atleast_1d(np.asarray(beta))
  Xb = np.dot(X, beta).astype(np.float64)
  Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent
  lam = np.exp(Xb_clipped)
  return np.sum(-lam + Y * Xb - gammaln(Y + 1))

def neg_loglike(beta, X, Y):
  return -poisson_loglike(beta, X, Y)

initial_beta = np.zeros(X.shape[1])
result = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')
beta_hat = result.x
hessian_inv = result.hess_inv
std_errs = np.sqrt(np.diag(hessian_inv))
summary = pd.DataFrame({
  "Coefficient": beta_hat,
  "Std. Error": std_errs
}, index=X.columns)

summary
```
::::

### Validate Results Using `statsmodels.GLM()`

To confirm the accuracy of our manual MLE implementation, we use `statsmodels.GLM()` to estimate the same Poisson regression model:

::::{.callout-note collapse="true"}
```{python}
import statsmodels.api as sm

# Drop 'intercept' column and ensure all data is float
X_glm = X.drop(columns='intercept', errors='ignore').astype(float)

# Add constant for intercept term
X_glm = sm.add_constant(X_glm)

# Fit GLM model
glm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())
glm_results = glm_model.fit()

# Display summary
glm_results.summary()

### Coefficients and Standard Errors from Poisson Regression
# Extract coefficient summary
coef_table = glm_results.summary2().tables[1][["Coef.", "Std.Err."]]
coef_table.rename(columns={"Coef.": "Coefficient", "Std.Err.": "Std. Error"}, inplace=True)

# Display table
coef_table
```
::::

### Interpretation 
### Key Regression Findings

- **Firm Age** shows a strong positive relationship with patent counts — older firms tend to file more patents.  
- **Age²** enters negatively and significantly, indicating **diminishing returns**: the effect of age on patent output tapers off as firms get older.  
- Use of **Blueprinty software** is associated with a **statistically significant increase** in patenting. The coefficient (0.2076, *p* < 0.001) suggests that Blueprinty users have an expected **23% more patents** than similar non-users.  
- **Regional indicators** (e.g., Northeast, Northwest) are **not statistically significant**, implying that once age and software use are controlled for, **location doesn't meaningfully affect patent outcomes**.

---

### Estimating Blueprinty’s Effect via Counterfactual Prediction

To isolate the impact of Blueprinty, we simulate two counterfactual scenarios using the fitted Poisson model:

- `X_0`: Set `iscustomer = 0` for all firms (i.e., assume no firm uses Blueprinty)  
- `X_1`: Set `iscustomer = 1` for all firms (i.e., assume all firms use Blueprinty)

We then predict the expected number of patents for each firm under both scenarios and compare the results. This approach highlights the **causal effect of software use**, holding other firm characteristics constant.

```{python}
# Make two versions of X_glm:
# X_0: all firms are non-customers
# X_1: all firms are customers
X_0 = X_glm.copy()
X_1 = X_glm.copy()

X_0["iscustomer"] = 0
X_1["iscustomer"] = 1

# Predict expected patent counts
y_pred_0 = glm_results.predict(X_0)
y_pred_1 = glm_results.predict(X_1)

# Estimate average treatment effect
average_effect = np.mean(y_pred_1 - y_pred_0)
```

### Interpretation
The average difference in predicted number of patents between Blueprinty customers and non-customers is:
```{python}
print(f"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}")
```

### Interpreting the Predicted Effect of Blueprinty

Based on counterfactual predictions, firms using Blueprinty's software are expected to file approximately **0.793 more patents** over a 5-year period than comparable non-users — after adjusting for age and region. This quantifies the **practical impact** of software adoption on innovation output.

---

## AirBnB Case Study

### Introduction

Airbnb is a widely-used platform for short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped data on **40,000 listings** in **New York City**. The dataset contains detailed information on listing features, pricing, reviews, and host behavior.

:::: {.callout-note collapse="true"}
### Variable Definitions

- `id`: Unique listing ID  
- `last_scraped`: Date the data was collected  
- `host_since`: Date the host first listed the unit  
- `days`: Duration listed on Airbnb (`last_scraped - host_since`)  
- `room_type`: Type of listing (Entire home/apt, Private room, Shared room)  
- `bathrooms`: Number of bathrooms  
- `bedrooms`: Number of bedrooms  
- `price`: Nightly rental price (USD)  
- `number_of_reviews`: Total review count for the listing  
- `review_scores_cleanliness`: Cleanliness rating (1–10)  
- `review_scores_location`: Location quality score (1–10)  
- `review_scores_value`: Value-for-money score (1–10)  
- `instant_bookable`: "t" if the unit can be booked instantly, "f" otherwise  
::::

### Data Cleaning

We begin by removing listings with missing values in any of the key variables. Following this, we conduct exploratory data analysis (EDA) to understand listing characteristics and prepare the dataset for further modeling.


### Drop Rows with Missing Data

```{python}
import pandas as pd
air_df = pd.read_csv("airbnb.csv")
relevant_cols = [
  "number_of_reviews", "room_type", "bathrooms", "bedrooms", "price", "days",
  "review_scores_cleanliness", "review_scores_location", "review_scores_value", "instant_bookable"
]
df_clean = air_df[relevant_cols].dropna()
```
### Exploratory Data Analysis (EDA)

### Distribution of Number of Reviews
```{python}
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.histplot(
    df_clean["number_of_reviews"],
    bins=50,
    kde=False,
    color="#1F78B4",

)

plt.title("Distribution of Number of Reviews", fontsize=14, weight="bold")
plt.xlabel("Number of Reviews", fontsize=12)
plt.ylabel("Count of Listings", fontsize=12)
plt.xlim(0, 100)
plt.grid(axis="y", linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()
```

### Average Number of Reviews by Room Type 
```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Data
avg_reviews = df_clean.groupby("room_type")["number_of_reviews"].mean().reset_index()

# Better-looking custom blue palette
custom_blue_palette = ["#A6CEE3", "#1F78B4", "#08519C"]

# Plot with warning suppression
with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=FutureWarning)

    plt.figure(figsize=(8, 6))
    sns.barplot(
        data=avg_reviews,
        x="room_type",
        y="number_of_reviews",
        palette=custom_blue_palette,
    )

plt.title("Average Number of Reviews by Room Type", fontsize=14, weight="bold")
plt.xlabel("Room Type", fontsize=12)
plt.ylabel("Average Number of Reviews", fontsize=12)
plt.xticks(rotation=0)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()
```

### Average Number of Reviews by Instant Bookability
```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
avg_reviews_by_bookable = df_clean.groupby("instant_bookable")["number_of_reviews"].mean().reset_index()
avg_reviews_by_bookable["instant_bookable"] = avg_reviews_by_bookable["instant_bookable"].map({"f": "No", "t": "Yes"})

blue_palette = {"No": "#6baed6", "Yes": "#2171b5"}
with warnings.catch_warnings():
    warnings.simplefilter("ignore")

    plt.figure(figsize=(8, 6))
    sns.barplot(
        data=avg_reviews_by_bookable,
        x="instant_bookable",
        y="number_of_reviews",
        hue="instant_bookable", 
        palette=blue_palette,
        legend=False,
    )
plt.title("Average Number of Reviews by Instant Bookability", fontsize=14, weight="bold")
plt.xlabel("Instant Bookable", fontsize=12)
plt.ylabel("Average Number of Reviews", fontsize=12)
plt.ylim(0, avg_reviews_by_bookable["number_of_reviews"].max() * 1.1)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()
```

### Correlation with Numeric Predictors
```{python}
import seaborn as sns
import matplotlib.pyplot as plt

numeric_vars = [
    "number_of_reviews", "bathrooms", "bedrooms", "price", "days",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value"
]
correlation_matrix = df_clean[numeric_vars].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(
    correlation_matrix,
    annot=True,
    fmt=".2f",
    cmap="Blues",             
    vmin=0, vmax=1,          
    square=True,
    linewidths=0.75,
    linecolor="white",
    annot_kws={"fontsize": 10, "weight": "bold"}
)
plt.title("Correlation Matrix of Numeric Variables", fontsize=14, weight="bold")
plt.xticks(rotation=45, ha="right", fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.tight_layout()
plt.show()
```

### Poisson Regression Model Using `statsmodels.GLM()`

We model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors:
- `room_type` (categorical)
- `instant_bookable` (binary)
- `price`, `days`, `bathrooms`, `bedrooms`
- Review scores: `cleanliness`, `location`, `value`

```{python}
import statsmodels.api as sm

room_dummies = pd.get_dummies(df_clean["room_type"], drop_first=True)
df_clean["instant_bookable"] = df_clean["instant_bookable"].map({"t": 1, "f": 0})

# Create design matrix
X = pd.concat([
    df_clean[["price", "days", "bathrooms", "bedrooms",
              "review_scores_cleanliness", "review_scores_location", "review_scores_value",
              "instant_bookable"]],
    room_dummies
], axis=1)

X = sm.add_constant(X)
X = X.astype(float)  

Y = df_clean["number_of_reviews"]

poisson_model = sm.GLM(Y, X, family=sm.families.Poisson())
poisson_results = poisson_model.fit()
summary_df = poisson_results.summary2().tables[1]

summary_df = summary_df.rename(columns={
    "Coef.": "Coefficient",
    "Std.Err.": "Std. Error",
    "P>|z|": "P-Value"
})

significant_results = summary_df[summary_df["P-Value"] < 0.05][["Coefficient", "Std. Error", "P-Value"]]

significant_results = significant_results.round(4)

significant_results
```

### Interpretation of Regression Coefficients

- **Intercept (`3.4980`)**  
  Represents the baseline log-expected number of reviews when all predictors are zero. Serves as a reference point.

- **Price (`-0.0000`)**  
  Slight negative effect: as price increases, listings receive marginally fewer reviews. This small but significant effect suggests higher prices may deter bookings.

- **Days Active (`+0.0001`)**  
  Listings that have been active longer accumulate more reviews — a reflection of greater visibility over time.

- **Bathrooms (`-0.1177`)**  
  Listings with more bathrooms tend to receive fewer reviews. This may indicate that higher-end or larger properties are booked less frequently.

- **Bedrooms (`+0.0741`)**  
  More bedrooms are associated with more reviews, likely due to the appeal of accommodating larger groups.

- **Cleanliness Score (`+0.1131`)**  
  Higher cleanliness ratings are linked to more reviews, emphasizing cleanliness as a key factor in guest satisfaction.

- **Location Score (`-0.0769`)**  
  A negative relationship, possibly due to limited variation in location scores or overlap with other variables.

- **Value Score (`-0.0911`)**  
  Listings with higher value ratings tend to receive fewer reviews, potentially reflecting patterns in less competitive markets.

- **Instant Bookable (`+0.3459`)**  
  Listings offering instant booking receive about **41% more reviews** than others:  
  \[
  \exp(0.3459) \approx 1.41
  \]

- **Private Room (`-0.0105`)**  
  Slightly fewer reviews than entire homes, possibly due to lower guest demand or preferences for full spaces.

- **Shared Room (`-0.2463`)**  
  Shared rooms receive significantly fewer reviews — roughly **22% less** than entire homes:  
  \[
  \exp(-0.2463) \approx 0.78
  \]

