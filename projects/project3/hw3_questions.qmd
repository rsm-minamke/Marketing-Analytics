---
title: "Multinomial Logit Model"
author: "Mrunmayee Inamke"
date: 05/27/2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.


```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

import numpy as np
import pandas as pd

# Set seed for reproducibility
np.random.seed(123)

# Define attributes
brand = ["N", "P", "H"]  # Netflix, Prime, Hulu
ad = ["Yes", "No"]
price = np.arange(8, 33, 4)  # $8 to $32 in $4 increments

# Generate all possible profiles
profiles = pd.DataFrame([
    {'brand': b, 'ad': a, 'price': p}
    for b in brand for a in ad for p in price
])
m = profiles.shape[0]

# Part-worth utilities (true parameters)
b_util = {"N": 1.0, "P": 0.5, "H": 0}
a_util = {"Yes": -0.8, "No": 0.0}
p_util = lambda p: -0.1 * p

# Configuration
n_peeps = 100
n_tasks = 10
n_alts = 3

# Function to simulate one respondentâ€™s data
def sim_one(id_):
    all_tasks = []
    for t in range(1, n_tasks + 1):
        sampled = profiles.sample(n=n_alts).copy()
        sampled["resp"] = id_
        sampled["task"] = t
        sampled["v"] = (
            sampled["brand"].map(b_util) +
            sampled["ad"].map(a_util) +
            p_util(sampled["price"])
        )
        # Add Gumbel (Type I Extreme Value) noise
        gumbel_noise = -np.log(-np.log(np.random.uniform(size=n_alts)))
        sampled["u"] = sampled["v"] + gumbel_noise
        sampled["choice"] = (sampled["u"] == sampled["u"].max()).astype(int)
        all_tasks.append(sampled)

    return pd.concat(all_tasks)

# Simulate data for all respondents
conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)])

# Keep only observable variables
conjoint_data = conjoint_data[["resp", "task", "brand", "ad", "price", "choice"]]

conjoint_data.head()

```

The output shows the first few rows of simulated conjoint data, where each row represents one product alternative shown to a respondent in a choice task. Key attributes include brand, ad presence, and price. Only one row per task has `choice = 1`, indicating the selected option based on utility.

## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.

#### Reshaping and Prepping the Data

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

import sklearn
from sklearn.preprocessing import OneHotEncoder


# Step 1: Encode categorical variables
categorical_cols = ['brand', 'ad']
encoder = OneHotEncoder(drop='first')  # no 'sparse' arg
encoded = encoder.fit_transform(conjoint_data[categorical_cols]).toarray()

# Step 2: Combine encoded categorical variables with numeric variables
X = np.hstack([encoded, conjoint_data[['price']].values])

# Step 3: Store structured data for estimation
mnl_data = {
    'X': X,
    'y': conjoint_data['choice'].values,
    'id': conjoint_data['resp'].values,
    'task': conjoint_data['task'].values
}

# Check dimensions
print(f"X shape: {mnl_data['X'].shape}")
print(f"y shape: {mnl_data['y'].shape}")

# # Preview reshaped X as a DataFrame
feature_names = encoder.get_feature_names_out(categorical_cols).tolist() + ['price']
X_df = pd.DataFrame(mnl_data['X'], columns=feature_names)
print(X_df.head())


```


## 4. Estimation via Maximum Likelihood

#### The log-likelihood function

To obtain the estimated coefficients of the Multinomial Logit (MNL) model, a log-likelihood function is defined based on individual-level choice data. For each choice task, the utility of each alternative is calculated as a linear function of its features and a parameter vector. These utilities are then normalized using the log-sum-exp trick to compute choice probabilities. The log-likelihood is formed by summing the log probabilities of the observed choices, and the negative of this value is minimized using the BFGS optimization algorithm. The result is a set of parameter estimates that maximize the likelihood of the observed choices, along with the log-likelihood value at the optimum.

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

from scipy.optimize import minimize
from scipy.special import logsumexp

# Step 1: Define the MNL log-likelihood function
def mnl_log_likelihood(beta, X, y, id_, task):
    beta = np.asarray(beta)
    utilities = X @ beta
    df = pd.DataFrame({
        'util': utilities,
        'choice': y,
        'id': id_,
        'task': task
    })
    df['log_denom'] = df.groupby(['id', 'task'])['util'].transform(logsumexp)
    df['log_prob'] = df['choice'] * (df['util'] - df['log_denom'])
    return -df['log_prob'].sum()

# Step 2: Set up and run the optimizer
K = mnl_data['X'].shape[1]
beta_init = np.zeros(K)  # Start from zero or small random values

result = minimize(
    fun=mnl_log_likelihood,
    x0=beta_init,
    args=(mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task']),
    method='BFGS'
)

# Step 3: Label and display results
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
estimates = pd.DataFrame({
    'Parameter': param_names,
    'Estimate': result.x
})

print("Estimated Coefficients:")
print(estimates.to_string(index=False))
print("\nLog-likelihood at optimum:")
print(-result.fun)

```


#### Extracting the MLEs and Standard Errors

Finding the MLEs for the 4 parameters ($\beta_\text{netflix}$, $\beta_\text{prime}$, $\beta_\text{ads}$, $\beta_\text{price}$), as well as their standard errors (from the Hessian).

Computing the standard error from the inverse of the Hessian matrix and constructing a 95% confidence interval for each parameter.

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

from scipy.optimize import minimize
from scipy.special import logsumexp


# Run optimization
result = minimize(
    fun=mnl_log_likelihood,
    x0=beta_init,
    args=(mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task']),
    method='BFGS',
    options={'disp': True}
)

# Extract MLEs
beta_hat = result.x

# Get standard errors from inverse Hessian
hessian_inv = result.hess_inv
if isinstance(hessian_inv, np.ndarray):
    se = np.sqrt(np.diag(hessian_inv))
else:  # if hess_inv is a BFGS object, convert to ndarray
    hessian_inv = hessian_inv.todense()
    se = np.sqrt(np.diag(hessian_inv))

# 95% confidence intervals
z = 1.96  # for 95% CI
lower = beta_hat - z * se
upper = beta_hat + z * se

# Output summary
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
summary = pd.DataFrame({
    'Parameter': param_names,
    'Estimate': beta_hat,
    'Std. Error': se,
    '95% CI Lower': lower,
    '95% CI Upper': upper
})

print(summary)
```

The output provides a detailed summary of the maximum likelihood estimates (MLEs) for the four parameters in the Multinomial Logit (MNL) model, along with their standard errors and 95% confidence intervals. These estimates quantify how each attribute (brand, ad presence, and price) affects the probability of a product being chosen.

1. `beta_netflix`:Holding ad presence and price constant, choosing Netflix increases the utility of a product by 1.06 units relative to the baseline brand (Hulu). This is a large and positive coefficient, indicating strong preference for Netflix. The 95% confidence interval [0.886,1.228] does not include 0, meaning the effect is statistically significant. This estimate is quite precise: the standard error is small relative to the estimate itself.
2. `beta_prime`: Again, holding ads and price constant, Amazon Prime is also preferred over Hulu, but less strongly than Netflix. It increases utility by 0.47 units, and the confidence interval [0.287,0.660] confirms this preference is also statistically significant. The estimate is still statistically significant, but the standard error is relatively larger than Netflixâ€™s.
2. `beta_ads`: Holding brand and price constant, the presence of advertisements decreases the utility of the product by about 0.77 units compared to an ad-free experience. This is a meaningful negative effect, and the confidence interval [âˆ’0.938,âˆ’0.607] shows this reduction in utility is statistically significant. Again, the standard error is small compared to the magnitude of the coefficient.
3. `beta_price`: Keeping brand and ad status constant, every $1 increase in monthly price reduces utility by about 0.096 units. This is a very small but precise estimate, with a very tight confidence interval [âˆ’0.108,âˆ’0.085]. This is the most precise estimate of all as the standard error is very small, which leads to a very tight confidence interval. This is consistent with standard economic intuition: higher prices reduce demand.

## 5. Estimation via Bayesian Methods

####  Metropolis-Hastings MCMC Sampler

Creating a metropolis-hasting MCMC sampler of the posterior distribution. Taking 11,000 steps and throwing away the first 1,000, retaining the subsequent 10,000.

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

def metropolis_hastings_mnl(n_iter=11000, burn_in=1000, proposal_sd=0.1):
    K = mnl_data['X'].shape[1]
    beta_curr = np.zeros(K)
    samples = []
    accepted = 0

    # Use negative log-likelihood, so log posterior = -nll
    curr_nll = mnl_log_likelihood(beta_curr, mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task'])

    for i in range(n_iter):
        beta_prop = beta_curr + np.random.normal(scale=proposal_sd, size=K)
        prop_nll = mnl_log_likelihood(beta_prop, mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task'])

        # Compute acceptance probability using log-likelihoods (note: negated)
        log_accept_ratio = -(prop_nll - curr_nll)
        if np.log(np.random.rand()) < log_accept_ratio:
            beta_curr = beta_prop
            curr_nll = prop_nll
            accepted += 1

        samples.append(beta_curr.copy())

        if (i + 1) % 1000 == 0:
            print(f"Step {i+1}, Acceptance Rate: {accepted / (i+1):.3f}")

    print(f"Final Acceptance Rate: {accepted / n_iter:.3f}")
    return np.array(samples[burn_in:])  # discard burn-in

# Run the sampler
posterior_samples = metropolis_hastings_mnl()

# Summarize posterior samples
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
posterior_df = pd.DataFrame(posterior_samples, columns=param_names)

print("\nPosterior means:")
print(posterior_df.mean())

print("\nPosterior standard deviations:")
print(posterior_df.std())
```

The output reflects the results of a Bayesian estimation of a Multinomial Logit (MNL) model using a Metropolis-Hastings MCMC sampler. The algorithm was run for 11,000 iterations, with the first 1,000 discarded as burn-in, yielding 10,000 posterior samples. The final acceptance rate was 4.3%, which is relatively low but not uncommon in MCMC when the proposal distribution is narrow. Despite the low acceptance, the posterior samples appeared to stabilize, suggesting the sampler was able to explore the target distribution effectively.

####  Updating the MCMC Sampler

Updating the MCMC Sampler Using N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.


```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

# Log-prior function for N(0, 5^2) for the first 3, and N(0, 1^2) for the price
def log_prior(beta):
    # First 3 are binary-related â†’ N(0, 25)
    log_prior_binary = -0.5 * np.sum((beta[:3] ** 2) / 25 + np.log(2 * np.pi * 25))
    # Last is price â†’ N(0, 1)
    log_prior_price = -0.5 * ((beta[3] ** 2) / 1 + np.log(2 * np.pi * 1))
    return log_prior_binary + log_prior_price

# Posterior = log-likelihood + log-prior
def log_posterior(beta, X, y, id_, task):
    return -mnl_log_likelihood(beta, X, y, id_, task) + log_prior(beta)

# Updated Metropolis-Hastings with Prior
def metropolis_hastings_posterior(n_iter=11000, burn_in=1000):
    K = mnl_data['X'].shape[1]
    beta_curr = np.zeros(K)
    samples = []
    accepted = 0

    curr_log_post = log_posterior(beta_curr, mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task'])

    for i in range(n_iter):
        # Propose new beta with independent draws:
        beta_prop = beta_curr + np.random.normal(loc=0, scale=[0.05, 0.05, 0.05, 0.005], size=K)
        prop_log_post = log_posterior(beta_prop, mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task'])

        # Accept with probability min(1, exp(new - old))
        log_accept_ratio = prop_log_post - curr_log_post
        if np.log(np.random.rand()) < log_accept_ratio:
            beta_curr = beta_prop
            curr_log_post = prop_log_post
            accepted += 1

        samples.append(beta_curr.copy())

        if (i + 1) % 1000 == 0:
            print(f"Step {i+1}, Acceptance Rate: {accepted / (i+1):.3f}")

    print(f"Final Acceptance Rate: {accepted / n_iter:.3f}")
    return np.array(samples[burn_in:])

# Run the posterior sampler
posterior_samples = metropolis_hastings_posterior()

# Summary
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
posterior_df = pd.DataFrame(posterior_samples, columns=param_names)

print("\nPosterior means with prior:")
print(posterior_df.mean())

print("\nPosterior standard deviations with prior:")
print(posterior_df.std())
```

This output summarizes the results of an updated Bayesian estimation using a Metropolis-Hastings MCMC sampler that incorporates informative Gaussian priors on the model parameters. Specifically, the first three parameters (beta_netflix, beta_prime, and beta_ads) use normal priors centered at 0 with variance 25 (i.e., N(0,5^2)), while the price coefficient (beta_price) uses a tighter prior, N(0,1), reflecting stronger prior belief in price sensitivity being closer to zero.

The results of the Bayesian estimation with informative priors show strong alignment with both the maximum likelihood estimates and the underlying true values used in the conjoint simulation. The posterior mean for beta_netflix is approximately 1.06, indicating that, holding price and ad presence constant, Netflix increases the utility of a product by over one full unit compared to Hulu. This is consistent with the simulated part-worth utility of 1.0 for Netflix. Similarly, beta_prime has a posterior mean of about 0.48, reflecting a positive but smaller preference for Amazon Prime over Hulu, closely matching its true value of 0.5.


#### Visualizing the Posterior Distribution

The trace plots of the algorithm and histogram of the posterior distribution for each of the four parameters will help us understand the convergence and distribution of the posterior samples.

##### Beta_Netflix

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

import matplotlib.pyplot as plt
import seaborn as sns

# Visualizing beta_netflix
plt.figure(figsize=(12, 4))

# Trace Plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df['beta_netflix'], color='tab:blue')
plt.title('Trace Plot: beta_netflix')
plt.xlabel('Iteration')
plt.ylabel('Value')

# Histogram of the Posterior
plt.subplot(1, 2, 2)
sns.histplot(posterior_df['beta_netflix'], bins=30, kde=True, color='tab:blue')
plt.title('Posterior Distribution: beta_netflix')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()
```


##### Beta_Prime
```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

plt.figure(figsize=(12, 4))

# Trace plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df['beta_prime'], color='tab:orange')
plt.title('Trace Plot: beta_prime')
plt.xlabel('Iteration')
plt.ylabel('Value')

# Histogram
plt.subplot(1, 2, 2)
sns.histplot(posterior_df['beta_prime'], bins=30, kde=True, color='tab:orange')
plt.title('Posterior Distribution: beta_prime')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()
```



##### Beta_Ads
```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

plt.figure(figsize=(12, 4))

# Trace plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df['beta_ads'], color='tab:green')
plt.title('Trace Plot: beta_ads')
plt.xlabel('Iteration')
plt.ylabel('Value')

# Histogram
plt.subplot(1, 2, 2)
sns.histplot(posterior_df['beta_ads'], bins=30, kde=True, color='tab:green')
plt.title('Posterior Distribution: beta_ads')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()
```


##### Beta_Price
```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

plt.figure(figsize=(12, 4))

# Trace plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df['beta_price'], color='tab:red')
plt.title('Trace Plot: beta_price')
plt.xlabel('Iteration')
plt.ylabel('Value')

# Histogram
plt.subplot(1, 2, 2)
sns.histplot(posterior_df['beta_price'], bins=30, kde=True, color='tab:red')
plt.title('Posterior Distribution: beta_price')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()
```



#### Comparing the posterior means, standard deviations, and 95% credible intervals to the results from the Maximum Likelihood approach

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

# Define parameter names
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']

# Calculate posterior summaries
posterior_summary = pd.DataFrame({
    'Parameter': param_names,
    'Mean': posterior_df.mean().values,
    'Std. Dev.': posterior_df.std().values,
    '2.5% CI': posterior_df.quantile(0.025).values,
    '97.5% CI': posterior_df.quantile(0.975).values
})

# Display the summary table
print(posterior_summary.round(4))
```


Hereâ€™s a comparison of the Bayesian posterior estimates to the results from the Maximum Likelihood Estimation (MLE) approach, focusing on the posterior means, standard deviations (uncertainty), and 95% credible intervals vs. confidence intervals.

beta_netflix: The posterior mean for beta_netflix is 1.0608, nearly identical to the MLE estimate of 1.0569. The standard deviation of the posterior is slightly larger (0.1103 vs. MLE SE of 0.0871), reflecting the added uncertainty introduced by incorporating prior beliefs. The 95% credible interval [0.8443,1.2723] is slightly wider than the MLE confidence interval [0.8863,1.2275], but both firmly support the conclusion that consumers strongly prefer Netflix.

beta_prime: The posterior mean for beta_prime is 0.4798, again very close to the MLE estimate of 0.4733. The posterior standard deviation is 0.1133, a bit higher than the MLE SE of 0.0951. The credible interval [0.2575,0.6980] slightly widens the uncertainty around the preference for Prime, but like the MLE, confirms it is positively valued compared to Hulu.

beta_ads: For beta_ads, the posterior mean is â€“0.7811, closely matching the MLE estimate of â€“0.7724. The posterior standard deviation is 0.0913, modestly higher than the MLEâ€™s 0.0846. The credible interval [â€“0.9511,â€“0.5979] is consistent with the MLE confidence interval [â€“0.9383,â€“0.6065], both confirming strong and statistically significant disutility from ad-supported content.

beta_price: The posterior mean of â€“0.0971 for beta_price is nearly identical to the MLE estimate of â€“0.0964. The posterior standard deviation (0.0062) is very close to the MLE SE (0.0061), and the 95% credible interval [â€“0.1090,â€“0.0854] aligns closely with the MLE confidence interval [â€“0.1083,â€“0.0845]. This shows high agreement between both methods in estimating price sensitivity with precision.

Across all four parameters, the posterior means are almost indistinguishable from the MLE estimates, validating the correctness and consistency of both estimation approaches. The posterior standard deviations are slightly higher than the MLE standard errors, which is expected since Bayesian estimation integrates over uncertainty rather than relying on local curvature. The credible intervals are slightly wider but overlap substantially with the MLE confidence intervals, reinforcing the reliability of the modelâ€™s findings under both frameworks.


## 6. Discussion

_todo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does $\beta_\text{Netflix} > \beta_\text{Prime}$ mean? Does it make sense that $\beta_\text{price}$ is negative?_

#### Interpreting Parameter Estimates Without Knowing the True Data-Generating Process and Exploring what $\beta_\text{Netflix} > \beta_\text{Prime} means

If we assume that the data was not simulated, meaning we are analyzing real-world consumer choice data rather than data generated with known "true" part-worths, we must interpret the parameter estimates based solely on their face value and what they imply about consumer behavior.

Overall, the parameter estimates seem internally consistent and economically rational. Even without knowing the true values from simulation, the model outputs are interpretable and actionable. They suggest that Netflix holds the strongest brand equity, that Prime is also favored over Hulu, that consumers dislike ads, and that they are somewhat sensitive to price, which are all insights that align well with typical expectations in the digital media space. These conclusions could inform product strategy, marketing, and pricing decisions if the data had come from an actual consumer conjoint survey.

#### Changes needed to simulate data 

To simulate data from and estimate the parameters of a multi-level or hierarchical Multinomial Logit (MNL) model (also known as a random-parameter logit), several key changes must be made to both the data-generating process and the estimation procedure. The standard MNL model assumes that all consumers share a single set of utility parameters (i.e., a single Î² vector). However, this assumption is often too restrictive for real-world data, where individuals have heterogeneous preferences. A hierarchical model relaxes this by allowing each consumer to have their own Î²_i, drawn from a common distribution.

Then, we would use each respondentâ€™s unique Î²_i to simulate their choice tasks. This approach better reflects individual-level preference variation and creates data with more realistic heterogeneity, especially useful when analyzing actual survey data.

For estimation, we can no longer use simple MLE or fixed-parameter Bayesian MCMC as done in this assignment. Instead, we would need to adopt a hierarchical Bayesian (HB) framework. This involves placing a prior on both the individual-level parameters (Î²_i) and the population-level hyperparameters (Î¼,Î£). 

Overall, to move from the basic MNL model to a hierarchical model, we would Simulate respondent-level Î²_i values from a population distribution when generating data. Replace single-level estimation methods with a hierarchical Bayesian estimation algorithm. Use the results to understand both average market trends and individual-level variationâ€”offering richer insights and better predictions in practice.

#### Estimating the parameters of a multi-level model (aka random-parameter or hierarchical) model

Assuming wach respondent has multiple choice tasks and extracting each respondentâ€™s design matrix and choice vector, we estimate individual-level coefficients Î²_i that vary across respondents, and sample the population-level mean (mu) and covariance (Sigma) of those coefficients, using Gibbs sampling, alternating between sampling respondent-level and population-level parameters.


```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

from numpy.linalg import inv, cholesky
from scipy.stats import invwishart, multivariate_normal

# individual level data
n_respondents = int(mnl_data['id'].max())
K = mnl_data['X'].shape[1]

# Group design matrix and choices by respondent
X_groups = [mnl_data['X'][mnl_data['id'] == i] for i in range(1, n_respondents+1)]
y_groups = [mnl_data['y'][mnl_data['id'] == i] for i in range(1, n_respondents+1)]
task_groups = [mnl_data['task'][mnl_data['id'] == i] for i in range(1, n_respondents+1)]


# Hierarchical Priors
mu_0 = np.zeros(K)
Sigma_0 = np.eye(K) * 10  # prior on mu
df = K + 2  # degrees of freedom for inverse-Wishart
scale_matrix = np.eye(K)  # scale matrix for Sigma prior

# Initialize
mu = np.zeros(K)
Sigma = np.eye(K)
beta_i = np.random.randn(n_respondents, K)

# Storage
draws_mu = []
draws_Sigma = []

# Helper: Individual Log-likelihood
def individual_log_likelihood(beta, X, y, task_ids):
    df = pd.DataFrame({'util': X @ beta, 'choice': y, 'task': task_ids})
    df['log_denom'] = df.groupby('task')['util'].transform(lambda u: np.log(np.sum(np.exp(u))))
    df['log_prob'] = df['choice'] * (df['util'] - df['log_denom'])
    return df['log_prob'].sum()


# Gibbs Sampling
n_iter = 1000
for iter in range(n_iter):
    # Step 1: Update beta_i for each respondent
    for i in range(n_respondents):
        X_i = X_groups[i]
        y_i = y_groups[i]
        task_i = task_groups[i]
        curr_beta = beta_i[i]
        
        # Metropolis step (local proposal)
        prop_beta = curr_beta + np.random.normal(scale=0.1, size=K)
        ll_curr = individual_log_likelihood(curr_beta, X_i, y_i, task_i)
        ll_prop = individual_log_likelihood(prop_beta, X_i, y_i, task_i)
        
        prior_curr = multivariate_normal.logpdf(curr_beta, mean=mu, cov=Sigma)
        prior_prop = multivariate_normal.logpdf(prop_beta, mean=mu, cov=Sigma)
        
        log_accept_ratio = (ll_prop + prior_prop) - (ll_curr + prior_curr)
        if np.log(np.random.rand()) < log_accept_ratio:
            beta_i[i] = prop_beta

    # Step 2: Update mu | beta_i, Sigma
    beta_bar = beta_i.mean(axis=0)
    mu_cov = inv(inv(Sigma_0) + n_respondents * inv(Sigma))
    mu_mean = mu_cov @ (inv(Sigma_0) @ mu_0 + n_respondents * inv(Sigma) @ beta_bar)
    mu = np.random.multivariate_normal(mu_mean, mu_cov)

    # Step 3: Update Sigma | beta_i, mu
    S = np.cov((beta_i - mu).T, bias=True)
    Sigma = invwishart.rvs(df=df + n_respondents, scale=scale_matrix + n_respondents * S)

    # Store draws
    draws_mu.append(mu)
    draws_Sigma.append(Sigma)

    if (iter + 1) % 100 == 0:
        print(f"Iteration {iter+1} completed.")

# Convert to DataFrames for summaries
draws_mu_df = pd.DataFrame(draws_mu, columns=['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price'])
print("\nPosterior means for mu:")
print(draws_mu_df.mean())

print("\nPosterior standard deviations for mu:")
print(draws_mu_df.std())

```

The output from your hierarchical Bayesian model presents the posterior distribution over the population-level means (Î¼) for each parameter in the Multinomial Logit model. These means reflect the average preference weights across all individuals in your sample, with uncertainty accounted for through respondent-level variation.

The posterior mean for Î²_prime is 0.24, again lower than the MLE (0.47) and flat-prior Bayesian (0.48) estimates. This too reflects the modelâ€™s flexibility: allowing for individual-level variation reveals that not all respondents prefer Prime equally. The relatively small standard deviation of 0.096 suggests that while preferences for Prime are generally weaker than Netflix, thereâ€™s less spread in how people feel about it.

For Î²_ads, the mean is â€“0.58, showing that on average, ad-supported content still reduces utility, as expected. However, the magnitude is smaller than in previous models (â€“0.77), and the larger standard deviation of 0.28 suggests greater heterogeneity in how respondents perceive ads. Some respondents may tolerate ads, while others strongly dislike them, and this variability is captured in the hierarchical framework.

Lastly, the posterior mean for Î²_price is â€“0.11, which is slightly more negative than in your earlier MLE (â€“0.096) and Bayesian (â€“0.097) estimates. This implies that, when accounting for individual-level preference variation, consumers may overall be slightly more price-sensitive than previously assumed. The standard deviation of 0.036 reflects relatively low heterogeneity â€” most respondents respond similarly to price changes.







