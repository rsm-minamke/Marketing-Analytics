---
title: "Multinomial Logit Model"
author: "Mrunmayee Inamke"
date: 05/27/2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

## Conjoint Simulation: Streaming Service Preferences

We simulate data from a **conjoint experiment** focused on video content streaming services. The setup includes:

- **100 respondents**,  
- Each completing **10 choice tasks**,  
- With **3 alternatives per task** (no “none” option — a choice is always made).

---

### Attribute Design

Each alternative represents a hypothetical streaming offer characterized by three attributes:

1. **Brand**: Netflix, Amazon Prime, or Hulu (Hulu as the reference level)  
2. **Ad Experience**: With ads or ad-free  
3. **Price**: Ranging from \$4 to \$32, in \$4 increments

---

### Part-Worth Utilities (Preference Weights)

The simulated utility \( u_{ij} \) for respondent *i* choosing option *j* is modeled as:

\[
u_{ij} = (1.0 \times \text{Netflix}_j) + (0.5 \times \text{Prime}_j) + (-0.8 \times \text{Ads}_j) + (-0.1 \times \text{Price}_j) + \varepsilon_{ij}
\]

Where:

- Brand indicators: Netflix and Amazon Prime (Hulu = reference)  
- Ads: 1 if ads are included, 0 if ad-free  
- Price: Monthly cost in dollars  
- \(\varepsilon_{ij}\): Random error term drawn from a **Type I Extreme Value** (Gumbel) distribution

This model reflects that respondents prefer:
- **Netflix most**, followed by **Prime**, and **Hulu least**
- **Ad-free experiences**, and
- **Lower prices**

---

### Data Generation

The following code simulates the full dataset based on this model, generating realistic respondent choices for use in discrete choice modeling or estimation.


```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

import numpy as np
import pandas as pd

# Set seed for reproducibility
np.random.seed(123)

# Define attributes
brand = ["N", "P", "H"]  # Netflix, Prime, Hulu
ad = ["Yes", "No"]
price = np.arange(8, 33, 4)  # $8 to $32 in $4 increments

# Generate all possible profiles
profiles = pd.DataFrame([
    {'brand': b, 'ad': a, 'price': p}
    for b in brand for a in ad for p in price
])
m = profiles.shape[0]

# Part-worth utilities (true parameters)
b_util = {"N": 1.0, "P": 0.5, "H": 0}
a_util = {"Yes": -0.8, "No": 0.0}
p_util = lambda p: -0.1 * p

# Configuration
n_peeps = 100
n_tasks = 10
n_alts = 3

# Function to simulate one respondent’s data
def sim_one(id_):
    all_tasks = []
    for t in range(1, n_tasks + 1):
        sampled = profiles.sample(n=n_alts).copy()
        sampled["resp"] = id_
        sampled["task"] = t
        sampled["v"] = (
            sampled["brand"].map(b_util) +
            sampled["ad"].map(a_util) +
            p_util(sampled["price"])
        )
        # Add Gumbel (Type I Extreme Value) noise
        gumbel_noise = -np.log(-np.log(np.random.uniform(size=n_alts)))
        sampled["u"] = sampled["v"] + gumbel_noise
        sampled["choice"] = (sampled["u"] == sampled["u"].max()).astype(int)
        all_tasks.append(sampled)

    return pd.concat(all_tasks)

# Simulate data for all respondents
conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)])

# Keep only observable variables
conjoint_data = conjoint_data[["resp", "task", "brand", "ad", "price", "choice"]]

conjoint_data.head()

```

The output displays the **first few rows** of the simulated conjoint dataset. Each row corresponds to a **single product alternative** presented to a respondent in a choice task.

Key variables include:
- `brand`: Streaming service brand (Netflix, Prime, or Hulu)
- `ad`: Whether the alternative includes ads
- `price`: Monthly subscription price
- `choice`: Equals `1` only for the **chosen alternative** in each task, based on calculated utility

Only **one row per task** will have `choice = 1`, reflecting the respondent’s selection among the three options.

---

## 3. Preparing the Data for Estimation

Before estimating the **Multinomial Logit (MNL)** model, we must properly structure the dataset.

Unlike standard cross-sectional regressions with just two dimensions (consumer *i*, covariate *k*), MNL models require tracking three:  
- **Respondent** (*i*)  
- **Alternative** (*j*)  
- **Attribute/Covariate** (*k*)

Fortunately, each choice task involves exactly **three alternatives**, simplifying this structure.

Additionally, we must:
- **One-hot encode categorical variables** such as `brand` (with Hulu as the reference) and `ads` (ad-free as the base),  
- Ensure that all variables are formatted as **numeric inputs** for the estimation procedure.

#### Reshaping and Prepping the Data

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

import sklearn
from sklearn.preprocessing import OneHotEncoder

# Step 1: Encode categorical variables
categorical_cols = ['brand', 'ad']
encoder = OneHotEncoder(drop='first')  # no 'sparse' arg
encoded = encoder.fit_transform(conjoint_data[categorical_cols]).toarray()

# Step 2: Combine encoded categorical variables with numeric variables
X = np.hstack([encoded, conjoint_data[['price']].values])

# Step 3: Store structured data for estimation
mnl_prep_data = {
    'X': X,
    'y': conjoint_data['choice'].values,
    'id': conjoint_data['resp'].values,
    'task': conjoint_data['task'].values
}

# Check dimensions
print(f"X shape: {mnl_prep_data['X'].shape}")
print(f"y shape: {mnl_prep_data['y'].shape}")

# # Preview reshaped X as a DataFrame
feature_names = encoder.get_feature_names_out(categorical_cols).tolist() + ['price']
X_df = pd.DataFrame(mnl_prep_data['X'], columns=feature_names)
print(X_df.head())


```


## 4. Estimation via Maximum Likelihood

### Log-Likelihood Function

To estimate the coefficients of the **Multinomial Logit (MNL)** model, we define a **log-likelihood function** using individual-level choice data.

For each choice task:
- The **utility** of each alternative is computed as a **linear combination** of its attributes and a vector of coefficients.
- These utilities are **normalized** using the **log-sum-exp trick** to ensure numerical stability and to calculate valid **choice probabilities**.

The **log-likelihood** is then constructed by summing the log of predicted probabilities for the **actual choices made** by respondents.

To estimate the parameters:
- We **minimize the negative log-likelihood** using the **BFGS optimization algorithm**, a quasi-Newton method.
- The optimization yields both the **parameter estimates** that best explain the observed data and the **log-likelihood value** at the optimal point.

This approach provides a statistically principled way to infer preference weights from discrete choice data.


```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

from scipy.optimize import minimize
from scipy.special import logsumexp

# Step 1: Define the MNL log-likelihood function
def mnl_log_likelihood(beta, X, y, id_, task):
    beta = np.asarray(beta)
    utilities = X @ beta
    df = pd.DataFrame({
        'util': utilities,
        'choice': y,
        'id': id_,
        'task': task
    })
    df['log_denom'] = df.groupby(['id', 'task'])['util'].transform(logsumexp)
    df['log_prob'] = df['choice'] * (df['util'] - df['log_denom'])
    return -df['log_prob'].sum()

# Step 2: Set up and run the optimizer
K = mnl_prep_data['X'].shape[1]
beta_init = np.zeros(K)  # Start from zero or small random values

result = minimize(
    fun=mnl_log_likelihood,
    x0=beta_init,
    args=(mnl_prep_data['X'], mnl_prep_data['y'], mnl_prep_data['id'], mnl_prep_data['task']),
    method='BFGS'
)

# Step 3: Label and display results
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
estimates = pd.DataFrame({
    'Parameter': param_names,
    'Estimate': result.x
})

print("Estimated Coefficients:")
print(estimates.to_string(index=False))
print("\nLog-likelihood at optimum:")
print(-result.fun)

```


### Extracting the MLEs and Standard Errors

After estimating the Multinomial Logit model, we extract the **Maximum Likelihood Estimates (MLEs)** for the four key parameters:

- \(\beta_{\text{netflix}}\)  
- \(\beta_{\text{prime}}\)  
- \(\beta_{\text{ads}}\)  
- \(\beta_{\text{price}}\)

To assess the precision of these estimates, we compute **standard errors** using the **inverse of the Hessian matrix** obtained at the optimum.

With these standard errors, we construct **95% confidence intervals** for each parameter estimate, allowing us to evaluate the statistical significance and uncertainty associated with each attribute's effect on choice.

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

from scipy.optimize import minimize
from scipy.special import logsumexp

# Run optimization
result = minimize(
    fun=mnl_log_likelihood,
    x0=beta_init,
    args=(mnl_prep_data['X'], mnl_prep_data['y'], mnl_prep_data['id'], mnl_prep_data['task']),
    method='BFGS',
    options={'disp': True}
)

# Extract MLEs
beta_hat = result.x

# Get standard errors from inverse Hessian
hessian_inv = result.hess_inv
if isinstance(hessian_inv, np.ndarray):
    se = np.sqrt(np.diag(hessian_inv))
else:  # if hess_inv is a BFGS object, convert to ndarray
    hessian_inv = hessian_inv.todense()
    se = np.sqrt(np.diag(hessian_inv))

# 95% confidence intervals
z = 1.96  # for 95% CI
lower = beta_hat - z * se
upper = beta_hat + z * se

# Output summary
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
summary = pd.DataFrame({
    'Parameter': param_names,
    'Estimate': beta_hat,
    'Std. Error': se,
    '95% CI Lower': lower,
    '95% CI Upper': upper
})

print(summary)
```

### Interpretation of Maximum Likelihood Estimates

The output provides a detailed summary of the **MLEs** for the four parameters in the **Multinomial Logit (MNL)** model, including **standard errors** and **95% confidence intervals**. These estimates capture how each attribute influences the probability of a product being chosen.

1. **`beta_netflix`**  
   Holding ads and price constant, choosing **Netflix** increases utility by **1.06 units** compared to Hulu (baseline).  
   - 95% CI: [0.886, 1.228] — does **not** include 0, indicating strong statistical significance  
   - Small standard error → **high precision**

2. **`beta_prime`**  
   Amazon Prime also increases utility relative to Hulu, though less than Netflix.  
   - Effect size: **+0.47 units**  
   - 95% CI: [0.287, 0.660] — statistically significant  
   - Slightly higher standard error, but still precise

3. **`beta_ads`**  
   Ads reduce utility by **0.77 units**, relative to an ad-free option.  
   - 95% CI: [−0.938, −0.607] — significant and meaningful negative effect  
   - Standard error is low → effect is estimated with confidence

4. **`beta_price`**  
   Each $1 increase in price reduces utility by **0.096 units**.  
   - 95% CI: [−0.108, −0.085] — **narrowest interval**, indicating high precision  
   - Consistent with economic theory: higher prices lower demand

---

## 5. Estimation via Bayesian Methods

### Metropolis-Hastings MCMC Sampler

We implement a **Metropolis-Hastings MCMC** sampler to draw from the **posterior distribution** of the model parameters.

- Total iterations: **11,000**  
- Burn-in period: **First 1,000** steps discarded  
- Retained draws: **10,000** posterior samples

This Bayesian approach allows for full probabilistic inference and quantifies parameter uncertainty through posterior distributions rather than relying solely on point estimates and standard errors.

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

def metropolis_hastings_mnl(n_iter=11000, burn_in=1000, proposal_sd=0.1):
    K = mnl_prep_data['X'].shape[1]
    beta_curr = np.zeros(K)
    samples = []
    accepted = 0

    # Use negative log-likelihood, so log posterior = -nll
    curr_nll = mnl_log_likelihood(beta_curr, mnl_prep_data['X'], mnl_prep_data['y'], mnl_prep_data['id'], mnl_prep_data['task'])

    for i in range(n_iter):
        beta_prop = beta_curr + np.random.normal(scale=proposal_sd, size=K)
        prop_nll = mnl_log_likelihood(beta_prop, mnl_prep_data['X'], mnl_prep_data['y'], mnl_prep_data['id'], mnl_prep_data['task'])

        # Compute acceptance probability using log-likelihoods (note: negated)
        log_accept_ratio = -(prop_nll - curr_nll)
        if np.log(np.random.rand()) < log_accept_ratio:
            beta_curr = beta_prop
            curr_nll = prop_nll
            accepted += 1

        samples.append(beta_curr.copy())

        if (i + 1) % 1000 == 0:
            print(f"Step {i+1}, Acceptance Rate: {accepted / (i+1):.3f}")

    print(f"Final Acceptance Rate: {accepted / n_iter:.3f}")
    return np.array(samples[burn_in:])  # discard burn-in

# Run the sampler
posterior_samples = metropolis_hastings_mnl()

# Summarize posterior samples
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
posterior_df = pd.DataFrame(posterior_samples, columns=param_names)

print("\nPosterior means:")
print(posterior_df.mean())

print("\nPosterior standard deviations:")
print(posterior_df.std())
```

### Results from Bayesian Estimation

The output summarizes the results of a **Bayesian estimation** of the Multinomial Logit (MNL) model using a **Metropolis-Hastings MCMC sampler**.

- **Total iterations**: 11,000  
- **Burn-in**: First 1,000 iterations discarded  
- **Posterior draws**: 10,000 samples retained  
- **Acceptance rate**: 4.3% — relatively low, but not unusual when the proposal distribution is narrow

Despite the low acceptance rate, the sampler showed good mixing and **convergence**. The posterior samples stabilized, indicating that the sampler successfully explored the **target distribution** and produced reliable estimates.

---

### Updating the MCMC Sampler

We update the prior assumptions for improved regularization:

- For binary attribute coefficients (`beta_netflix`, `beta_prime`, `beta_ads`):  
  Use **Normal(0, 5)** priors  
- For the price coefficient (`beta_price`):  
  Use a **more informative Normal(0, 1)** prior, reflecting tighter beliefs about the sensitivity to price

These updated priors help guide the MCMC process while still allowing sufficient flexibility in posterior inference.

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

# Log-prior function for N(0, 5^2) for the first 3, and N(0, 1^2) for the price
def log_prior(beta):
    # First 3 are binary-related → N(0, 25)
    log_prior_binary = -0.5 * np.sum((beta[:3] ** 2) / 25 + np.log(2 * np.pi * 25))
    # Last is price → N(0, 1)
    log_prior_price = -0.5 * ((beta[3] ** 2) / 1 + np.log(2 * np.pi * 1))
    return log_prior_binary + log_prior_price

# Posterior = log-likelihood + log-prior
def log_posterior(beta, X, y, id_, task):
    return -mnl_log_likelihood(beta, X, y, id_, task) + log_prior(beta)

# Updated Metropolis-Hastings with Prior
def metropolis_hastings_posterior(n_iter=11000, burn_in=1000):
    K = mnl_prep_data['X'].shape[1]
    beta_curr = np.zeros(K)
    samples = []
    accepted = 0

    curr_log_post = log_posterior(beta_curr, mnl_prep_data['X'], mnl_prep_data['y'], mnl_prep_data['id'], mnl_prep_data['task'])

    for i in range(n_iter):
        # Propose new beta with independent draws:
        beta_prop = beta_curr + np.random.normal(loc=0, scale=[0.05, 0.05, 0.05, 0.005], size=K)
        prop_log_post = log_posterior(beta_prop, mnl_prep_data['X'], mnl_prep_data['y'], mnl_prep_data['id'], mnl_prep_data['task'])

        # Accept with probability min(1, exp(new - old))
        log_accept_ratio = prop_log_post - curr_log_post
        if np.log(np.random.rand()) < log_accept_ratio:
            beta_curr = beta_prop
            curr_log_post = prop_log_post
            accepted += 1

        samples.append(beta_curr.copy())

        if (i + 1) % 1000 == 0:
            print(f"Step {i+1}, Acceptance Rate: {accepted / (i+1):.3f}")

    print(f"Final Acceptance Rate: {accepted / n_iter:.3f}")
    return np.array(samples[burn_in:])

# Run the posterior sampler
posterior_samples = metropolis_hastings_posterior()

# Summary
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
posterior_df = pd.DataFrame(posterior_samples, columns=param_names)

print("\nPosterior means with prior:")
print(posterior_df.mean())

print("\nPosterior standard deviations with prior:")
print(posterior_df.std())
```

### Updated Bayesian Estimation with Informative Priors

This output summarizes results from an updated **Bayesian estimation** of the Multinomial Logit (MNL) model using a **Metropolis-Hastings MCMC sampler** with **informative Gaussian priors** on model parameters.

- For `beta_netflix`, `beta_prime`, and `beta_ads`:  
  \[
  \text{Prior} \sim \mathcal{N}(0, 5^2)
  \]
- For `beta_price`:  
  \[
  \text{Prior} \sim \mathcal{N}(0, 1)
  \]  
  This reflects stronger prior belief that price sensitivity is likely close to zero.

---

### Posterior Summary and Interpretation

The posterior estimates **align closely** with both the **maximum likelihood estimates (MLEs)** and the **true parameter values** used in the simulation.

- **`beta_netflix`**: Posterior mean ≈ **1.06**, indicating that, holding ads and price constant, Netflix increases utility by over one unit compared to Hulu. This matches the true part-worth value (1.0).
- **`beta_prime`**: Posterior mean ≈ **0.48**, showing a smaller but still positive preference for Amazon Prime over Hulu, consistent with the true value of 0.5.

These results confirm the reliability of the Bayesian estimation procedure and validate the simulated model structure.

#### Visualizing the Posterior Distribution

The trace plots of the algorithm and histogram of the posterior distribution for each of the four parameters will help us understand the convergence and distribution of the posterior samples.

##### Beta_Netflix

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

import matplotlib.pyplot as plt
import seaborn as sns

# Visualizing beta_netflix
plt.figure(figsize=(12, 4))

# Trace Plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df['beta_netflix'], color='tab:blue')
plt.title('Trace Plot: beta_netflix')
plt.xlabel('Iteration')
plt.ylabel('Value')

# Histogram of the Posterior
plt.subplot(1, 2, 2)
sns.histplot(posterior_df['beta_netflix'], bins=30, kde=True, color='tab:blue')
plt.title('Posterior Distribution: beta_netflix')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()
```


##### Beta_Prime
```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

plt.figure(figsize=(12, 4))

# Trace plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df['beta_prime'], color='tab:orange')
plt.title('Trace Plot: beta_prime')
plt.xlabel('Iteration')
plt.ylabel('Value')

# Histogram
plt.subplot(1, 2, 2)
sns.histplot(posterior_df['beta_prime'], bins=30, kde=True, color='tab:orange')
plt.title('Posterior Distribution: beta_prime')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()
```



##### Beta_Ads
```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

plt.figure(figsize=(12, 4))

# Trace plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df['beta_ads'], color='tab:green')
plt.title('Trace Plot: beta_ads')
plt.xlabel('Iteration')
plt.ylabel('Value')

# Histogram
plt.subplot(1, 2, 2)
sns.histplot(posterior_df['beta_ads'], bins=30, kde=True, color='tab:green')
plt.title('Posterior Distribution: beta_ads')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()
```


##### Beta_Price
```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

plt.figure(figsize=(12, 4))

# Trace plot
plt.subplot(1, 2, 1)
plt.plot(posterior_df['beta_price'], color='tab:red')
plt.title('Trace Plot: beta_price')
plt.xlabel('Iteration')
plt.ylabel('Value')

# Histogram
plt.subplot(1, 2, 2)
sns.histplot(posterior_df['beta_price'], bins=30, kde=True, color='tab:red')
plt.title('Posterior Distribution: beta_price')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()
```



#### Comparing the posterior means, standard deviations, and 95% credible intervals to the results from the Maximum Likelihood approach

```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

# Define parameter names
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']

# Calculate posterior summaries
posterior_summary = pd.DataFrame({
    'Parameter': param_names,
    'Mean': posterior_df.mean().values,
    'Std. Dev.': posterior_df.std().values,
    '2.5% CI': posterior_df.quantile(0.025).values,
    '97.5% CI': posterior_df.quantile(0.975).values
})

# Display the summary table
print(posterior_summary.round(4))
```


### Comparison: Bayesian Posterior vs. Maximum Likelihood Estimates

We compare the **Bayesian posterior estimates** with those from **Maximum Likelihood Estimation (MLE)**, focusing on posterior means, uncertainty (standard deviations vs. standard errors), and 95% **credible intervals** versus **confidence intervals**.

#### `beta_netflix`

- **Posterior mean**: 1.0608  
- **MLE estimate**: 1.0569  
- **Posterior SD**: 0.1103 vs. **MLE SE**: 0.0871  
- **95% Credible Interval**: [0.8443, 1.2723]  
- **95% Confidence Interval**: [0.8863, 1.2275]

 Both methods indicate a strong preference for Netflix, with slightly wider credible intervals due to the inclusion of prior uncertainty.

#### `beta_prime`

- **Posterior mean**: 0.4798  
- **MLE estimate**: 0.4733  
- **Posterior SD**: 0.1133 vs. **MLE SE**: 0.0951  
- **95% Credible Interval**: [0.2575, 0.6980]  
- **95% Confidence Interval**: [0.2870, 0.6600]

 Results again align closely, with slightly more uncertainty in the Bayesian estimate. Both confirm a positive utility for Prime relative to Hulu.

#### `beta_ads`

- **Posterior mean**: –0.7811  
- **MLE estimate**: –0.7724  
- **Posterior SD**: 0.0913 vs. **MLE SE**: 0.0846  
- **95% Credible Interval**: [–0.9511, –0.5979]  
- **95% Confidence Interval**: [–0.9383, –0.6065]

 Both estimates show a significant negative impact of advertisements, with excellent agreement between estimation methods.

#### `beta_price`

- **Posterior mean**: –0.0971  
- **MLE estimate**: –0.0964  
- **Posterior SD**: 0.0062 vs. **MLE SE**: 0.0061  
- **95% Credible Interval**: [–0.1090, –0.0854]  
- **95% Confidence Interval**: [–0.1083, –0.0845]

High consistency in estimating price sensitivity; both approaches yield nearly identical results with tight uncertainty bounds.

---

### Summary

Across all four parameters, the **Bayesian posterior means closely match the MLE estimates**, confirming the correctness and robustness of both approaches. As expected, **posterior standard deviations** are slightly larger than MLE standard errors, reflecting the integration of prior uncertainty. The **credible intervals** are modestly wider but substantially overlap with confidence intervals, reinforcing the **consistency and reliability** of the model’s insights under both frequentist and Bayesian frameworks.


## 6. Discussion

_todo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does $\beta_\text{Netflix} > \beta_\text{Prime}$ mean? Does it make sense that $\beta_\text{price}$ is negative?_

### Interpreting Estimates Without Knowing the True Data-Generating Process

If we assume the data is from a **real-world consumer choice study** (not simulated), we interpret the parameter estimates based on observed patterns rather than known "true" values.

Despite not knowing the underlying data-generating process, the model’s results appear **internally consistent and economically plausible**:
- **`β_netflix > β_prime`** indicates stronger consumer preference for Netflix over Amazon Prime.
- Both brands are preferred to Hulu, the baseline.
- **Ads** reduce utility, and **higher prices** deter choices — consistent with common expectations.

These insights align well with industry intuition and could meaningfully inform **product strategy**, **brand positioning**, and **pricing decisions** in digital streaming markets.

---

### Simulating Data for a Hierarchical Multinomial Logit (MNL) Model

To simulate data for a **multi-level (hierarchical) MNL model**, several key changes are required:

- The standard MNL assumes all consumers share a **single β vector**. This is often unrealistic.
- A **hierarchical model** allows each respondent to have their own preferences, modeled as:
  \[
  \beta_i \sim \mathcal{N}(\mu, \Sigma)
  \]
- We simulate each respondent’s choices using their unique \(\beta_i\), better capturing **preference heterogeneity**.

This structure is particularly useful for **realistic conjoint data**, where different users value features differently.

---

### Estimating Parameters in a Hierarchical (Random-Coefficient) Logit Model

To estimate such a model, we:

1. **Extract** each respondent’s choice data and corresponding design matrix.
2. **Estimate individual-level β vectors** that vary across respondents.
3. Use **Gibbs sampling** to alternate between:
   - Sampling **individual-level coefficients** (\(\beta_i\))  
   - Sampling **population-level parameters** — the mean vector (\(\mu\)) and covariance matrix (\(\Sigma\))

This **Hierarchical Bayesian (HB)** approach allows us to:
- Capture **both market-level trends** and **individual-level variation**
- Generate richer, more accurate predictions
- Offer deeper insights into **preference heterogeneity** across consumers


```{python}
#| code-summary: "Click to show/hide this code"
#| echo: true

from numpy.linalg import inv, cholesky
from scipy.stats import invwishart, multivariate_normal

# individual level data
n_respondents = int(mnl_prep_data['id'].max())
K = mnl_prep_data['X'].shape[1]

# Group design matrix and choices by respondent
X_groups = [mnl_prep_data['X'][mnl_prep_data['id'] == i] for i in range(1, n_respondents+1)]
y_groups = [mnl_prep_data['y'][mnl_prep_data['id'] == i] for i in range(1, n_respondents+1)]
task_groups = [mnl_prep_data['task'][mnl_prep_data['id'] == i] for i in range(1, n_respondents+1)]


# Hierarchical Priors
mu_0 = np.zeros(K)
Sigma_0 = np.eye(K) * 10  # prior on mu
df = K + 2  # degrees of freedom for inverse-Wishart
scale_matrix = np.eye(K)  # scale matrix for Sigma prior

# Initialize
mu = np.zeros(K)
Sigma = np.eye(K)
beta_i = np.random.randn(n_respondents, K)

# Storage
draws_mu = []
draws_Sigma = []

# Helper: Individual Log-likelihood
def individual_log_likelihood(beta, X, y, task_ids):
    df = pd.DataFrame({'util': X @ beta, 'choice': y, 'task': task_ids})
    df['log_denom'] = df.groupby('task')['util'].transform(lambda u: np.log(np.sum(np.exp(u))))
    df['log_prob'] = df['choice'] * (df['util'] - df['log_denom'])
    return df['log_prob'].sum()


# Gibbs Sampling
n_iter = 1000
for iter in range(n_iter):
    # Step 1: Update beta_i for each respondent
    for i in range(n_respondents):
        X_i = X_groups[i]
        y_i = y_groups[i]
        task_i = task_groups[i]
        curr_beta = beta_i[i]
        
        # Metropolis step (local proposal)
        prop_beta = curr_beta + np.random.normal(scale=0.1, size=K)
        ll_curr = individual_log_likelihood(curr_beta, X_i, y_i, task_i)
        ll_prop = individual_log_likelihood(prop_beta, X_i, y_i, task_i)
        
        prior_curr = multivariate_normal.logpdf(curr_beta, mean=mu, cov=Sigma)
        prior_prop = multivariate_normal.logpdf(prop_beta, mean=mu, cov=Sigma)
        
        log_accept_ratio = (ll_prop + prior_prop) - (ll_curr + prior_curr)
        if np.log(np.random.rand()) < log_accept_ratio:
            beta_i[i] = prop_beta

    # Step 2: Update mu | beta_i, Sigma
    beta_bar = beta_i.mean(axis=0)
    mu_cov = inv(inv(Sigma_0) + n_respondents * inv(Sigma))
    mu_mean = mu_cov @ (inv(Sigma_0) @ mu_0 + n_respondents * inv(Sigma) @ beta_bar)
    mu = np.random.multivariate_normal(mu_mean, mu_cov)

    # Step 3: Update Sigma | beta_i, mu
    S = np.cov((beta_i - mu).T, bias=True)
    Sigma = invwishart.rvs(df=df + n_respondents, scale=scale_matrix + n_respondents * S)

    # Store draws
    draws_mu.append(mu)
    draws_Sigma.append(Sigma)

    if (iter + 1) % 100 == 0:
        print(f"Iteration {iter+1} completed.")

# Convert to DataFrames for summaries
draws_mu_df = pd.DataFrame(draws_mu, columns=['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price'])
print("\nPosterior means for mu:")
print(draws_mu_df.mean())

print("\nPosterior standard deviations for mu:")
print(draws_mu_df.std())

```

### Interpreting Output from the Hierarchical Bayesian Model

The output from the **hierarchical Bayesian Multinomial Logit model** summarizes the **posterior distribution of the population-level means** (\(\mu\)) for each parameter. These reflect the **average preference weights** across all individuals, while accounting for **respondent-level heterogeneity**.

---

#### `β_prime`

- **Posterior mean**: 0.24  
- This is **lower** than the MLE (0.47) and flat-prior Bayesian estimate (0.48), highlighting the flexibility of the hierarchical model.
- A **standard deviation** of 0.096 suggests moderate consensus among respondents — while **preferences for Prime are generally positive**, they are weaker and **less varied** compared to Netflix.

---

#### `β_ads`

- **Posterior mean**: –0.58  
- Still negative (as expected), but **less extreme** than the MLE/flat Bayesian estimates (≈ –0.77).
- A **larger standard deviation** of 0.28 reveals greater **heterogeneity** in how respondents view ads.  
  → Some users are more tolerant of ads, while others strongly dislike them — this variation is captured by the hierarchical structure.

---

#### `β_price`

- **Posterior mean**: –0.11  
- Slightly **more negative** than the earlier estimates (≈ –0.096 to –0.097), suggesting **greater overall price sensitivity**.
- **Standard deviation**: 0.036  
  → Indicates relatively **low variation** — most respondents react **similarly to price**, even when individual preferences are allowed to vary.

---

These results demonstrate the **value of hierarchical modeling** in uncovering both **average effects** and **individual-level diversity**, enabling more nuanced insights than traditional models.
