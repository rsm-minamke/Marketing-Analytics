[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\nMrunmayee Inamke\nJun 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdd Title\n\n\n\n\nYour Name\nJun 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\nMrunmayee Inamke\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nMrunmayee Inamke\nJun 8, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project2/hw2_questions.html",
    "href": "projects/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, multiple=\"dodge\")\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\nFirms that are customers of Blueprinty tend to have more patents on average (≈ 4.13) than non-customers (≈ 3.47). The histogram shows that customer firms are more concentrated at higher patent counts, suggesting a potential positive relationship between using Blueprinty and patent success. However, this pattern may also be influenced by other factors, such as region or firm age, which need to be explored further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npalette = {0: \"#4C72B0\", 1: \"#DD8452\"}\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\", palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Map customer status labels\ndf[\"Customer Status\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Define matching palette\npalette = {\"Non-Customer\": \"#4C72B0\", \"Customer\": \"#DD8452\"}\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x=\"Customer Status\", y=\"age\", hue=\"Customer Status\", palette=palette, legend=False)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\n\nThe regional distribution of firms is not uniform between customers and non-customers. Some regions (e.g., Northeast) have a higher concentration of Blueprinty customers. This suggests that region may confound the relationship between software usage and patent outcomes.\nRegarding age, customers are slightly older on average (~26.9 years) than non-customers (~26.1 years), though the difference is modest. It is still important to consider firm age in the analysis to avoid biased conclusions.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nProbability Mass Function\nThe probability mass function for a single observation from a Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nLikelihood Function\nAssuming the observations are independent, the likelihood function for the entire dataset is:\n\\[\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis can be rewritten as:\n\\[\nL(\\lambda) = e^{-n\\lambda} \\cdot \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]\nLog-Likelihood Function\nTaking the natural logarithm of the likelihood gives:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nThis log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\n\n\n\n\nWe visualize the Poisson log-likelihood as a function of ( ), where the maximum corresponds to the MLE.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nWe can simplify this (since ( Y_i! ) does not depend on ( )):\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n\\]\nStep 2: First Derivative\nTake the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nStep 3: Set Derivative to Zero\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average.\n\n\n\nWe use numerical optimization to find the value of ( ) that maximizes the Poisson log-likelihood.\n\n\n\n\n\n\n\n\nCode\nfrom scipy import optimize\nneg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)\nresult = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n\n3.6846667021660804\n\n\n\n\n\n\n\n\nThe maximum likelihood estimate (MLE) of ( ) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.\n\n\nCode\n# Compare with the sample mean\ndf[\"patents\"].mean()\n\n\n3.6846666666666668\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson regression log-likelihood function\ndef poisson_regression_loglike(beta, X, Y):\n    Xbeta = X @ beta\n    lambdas = np.exp(Xbeta)\n    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create age squared\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create region dummies (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[\"age\"],\n    df[\"age2\"],\n    region_dummies,\n    df[\"iscustomer\"]\n], axis=1)\n\nY = df[\"patents\"].values\nX_matrix = X.values\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent\n    lam = np.exp(Xb_clipped)\n\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\nsummary\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509955\n0.193041\n\n\nage\n0.148702\n0.014461\n\n\nage2\n-0.002972\n0.000266\n\n\nNortheast\n0.029159\n0.046761\n\n\nNorthwest\n-0.017578\n0.057233\n\n\nSouth\n0.056567\n0.056243\n\n\nSouthwest\n0.050589\n0.049616\n\n\niscustomer\n0.207600\n0.032938\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the accuracy of our manual MLE implementation, we use statsmodels.GLM() to estimate the same Poisson regression model:\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n### Coefficients and Standard Errors from Poisson Regression\n# Extract coefficient summary\ncoef_table = glm_results.summary2().tables[1][[\"Coef.\", \"Std.Err.\"]]\ncoef_table.rename(columns={\"Coef.\": \"Coefficient\", \"Std.Err.\": \"Std. Error\"}, inplace=True)\n\n# Display table\ncoef_table\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nconst\n-0.508920\n0.183179\n\n\nage\n0.148619\n0.013869\n\n\nage2\n-0.002970\n0.000258\n\n\nNortheast\n0.029170\n0.043625\n\n\nNorthwest\n-0.017575\n0.053781\n\n\nSouth\n0.056561\n0.052662\n\n\nSouthwest\n0.050576\n0.047198\n\n\niscustomer\n0.207591\n0.030895\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge has a strong positive effect on patent activity: older firms are more likely to have more patents.\nAge² is negative and significant, suggesting a diminishing return — patent output increases with age but at a decreasing rate.\nBlueprinty’s software has a statistically significant positive coefficient of 0.2076 (p &lt; 0.001). This implies firms using the software are expected to have 23% more patents than comparable non-customers.\nRegional effects (Northeast, Northwest, etc.) are not statistically significant, suggesting location does not materially affect patent outcomes once other factors are controlled for.\n\nEstimate Effect of Blueprinty’s Software via Counterfactual Prediction\nWe simulate two scenarios:\n\nX_0: All firms set to non-customer (iscustomer = 0)\nX_1: All firms set to customer (iscustomer = 1)\n\nThen we compare the predicted number of patents for each firm under the two scenarios using the fitted model.\n\n\nCode\n# Make two versions of X_glm:\n# X_0: all firms are non-customers\n# X_1: all firms are customers\nX_0 = X_glm.copy()\nX_1 = X_glm.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Estimate average treatment effect\naverage_effect = np.mean(y_pred_1 - y_pred_0)\n\n\n\n\n\nThe average difference in predicted number of patents between Blueprinty customers and non-customers is:\n\n\nCode\nprint(f\"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}\")\n\n\nEstimated average increase in patent count from using Blueprinty: 0.793\n\n\nThis quantifies the effect of Blueprinty’s software: firms using it are predicted to file approximately 0.793 more patents over 5 years, on average, than similar firms who don’t use it, controlling for age and region."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, multiple=\"dodge\")\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\nFirms that are customers of Blueprinty tend to have more patents on average (≈ 4.13) than non-customers (≈ 3.47). The histogram shows that customer firms are more concentrated at higher patent counts, suggesting a potential positive relationship between using Blueprinty and patent success. However, this pattern may also be influenced by other factors, such as region or firm age, which need to be explored further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npalette = {0: \"#4C72B0\", 1: \"#DD8452\"}\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\", palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Map customer status labels\ndf[\"Customer Status\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Define matching palette\npalette = {\"Non-Customer\": \"#4C72B0\", \"Customer\": \"#DD8452\"}\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x=\"Customer Status\", y=\"age\", hue=\"Customer Status\", palette=palette, legend=False)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\n\nThe regional distribution of firms is not uniform between customers and non-customers. Some regions (e.g., Northeast) have a higher concentration of Blueprinty customers. This suggests that region may confound the relationship between software usage and patent outcomes.\nRegarding age, customers are slightly older on average (~26.9 years) than non-customers (~26.1 years), though the difference is modest. It is still important to consider firm age in the analysis to avoid biased conclusions.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nProbability Mass Function\nThe probability mass function for a single observation from a Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nLikelihood Function\nAssuming the observations are independent, the likelihood function for the entire dataset is:\n\\[\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis can be rewritten as:\n\\[\nL(\\lambda) = e^{-n\\lambda} \\cdot \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]\nLog-Likelihood Function\nTaking the natural logarithm of the likelihood gives:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nThis log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\n\n\n\n\nWe visualize the Poisson log-likelihood as a function of ( ), where the maximum corresponds to the MLE.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nWe can simplify this (since ( Y_i! ) does not depend on ( )):\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n\\]\nStep 2: First Derivative\nTake the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nStep 3: Set Derivative to Zero\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average.\n\n\n\nWe use numerical optimization to find the value of ( ) that maximizes the Poisson log-likelihood.\n\n\n\n\n\n\n\n\nCode\nfrom scipy import optimize\nneg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)\nresult = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n\n3.6846667021660804\n\n\n\n\n\n\n\n\nThe maximum likelihood estimate (MLE) of ( ) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.\n\n\nCode\n# Compare with the sample mean\ndf[\"patents\"].mean()\n\n\n3.6846666666666668\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson regression log-likelihood function\ndef poisson_regression_loglike(beta, X, Y):\n    Xbeta = X @ beta\n    lambdas = np.exp(Xbeta)\n    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create age squared\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create region dummies (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[\"age\"],\n    df[\"age2\"],\n    region_dummies,\n    df[\"iscustomer\"]\n], axis=1)\n\nY = df[\"patents\"].values\nX_matrix = X.values\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent\n    lam = np.exp(Xb_clipped)\n\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\nsummary\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509955\n0.193041\n\n\nage\n0.148702\n0.014461\n\n\nage2\n-0.002972\n0.000266\n\n\nNortheast\n0.029159\n0.046761\n\n\nNorthwest\n-0.017578\n0.057233\n\n\nSouth\n0.056567\n0.056243\n\n\nSouthwest\n0.050589\n0.049616\n\n\niscustomer\n0.207600\n0.032938\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the accuracy of our manual MLE implementation, we use statsmodels.GLM() to estimate the same Poisson regression model:\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n### Coefficients and Standard Errors from Poisson Regression\n# Extract coefficient summary\ncoef_table = glm_results.summary2().tables[1][[\"Coef.\", \"Std.Err.\"]]\ncoef_table.rename(columns={\"Coef.\": \"Coefficient\", \"Std.Err.\": \"Std. Error\"}, inplace=True)\n\n# Display table\ncoef_table\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nconst\n-0.508920\n0.183179\n\n\nage\n0.148619\n0.013869\n\n\nage2\n-0.002970\n0.000258\n\n\nNortheast\n0.029170\n0.043625\n\n\nNorthwest\n-0.017575\n0.053781\n\n\nSouth\n0.056561\n0.052662\n\n\nSouthwest\n0.050576\n0.047198\n\n\niscustomer\n0.207591\n0.030895\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge has a strong positive effect on patent activity: older firms are more likely to have more patents.\nAge² is negative and significant, suggesting a diminishing return — patent output increases with age but at a decreasing rate.\nBlueprinty’s software has a statistically significant positive coefficient of 0.2076 (p &lt; 0.001). This implies firms using the software are expected to have 23% more patents than comparable non-customers.\nRegional effects (Northeast, Northwest, etc.) are not statistically significant, suggesting location does not materially affect patent outcomes once other factors are controlled for.\n\nEstimate Effect of Blueprinty’s Software via Counterfactual Prediction\nWe simulate two scenarios:\n\nX_0: All firms set to non-customer (iscustomer = 0)\nX_1: All firms set to customer (iscustomer = 1)\n\nThen we compare the predicted number of patents for each firm under the two scenarios using the fitted model.\n\n\nCode\n# Make two versions of X_glm:\n# X_0: all firms are non-customers\n# X_1: all firms are customers\nX_0 = X_glm.copy()\nX_1 = X_glm.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Estimate average treatment effect\naverage_effect = np.mean(y_pred_1 - y_pred_0)\n\n\n\n\n\nThe average difference in predicted number of patents between Blueprinty customers and non-customers is:\n\n\nCode\nprint(f\"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}\")\n\n\nEstimated average increase in patent count from using Blueprinty: 0.793\n\n\nThis quantifies the effect of Blueprinty’s software: firms using it are predicted to file approximately 0.793 more patents over 5 years, on average, than similar firms who don’t use it, controlling for age and region."
  },
  {
    "objectID": "projects/project2/hw2_questions.html#airbnb-case-study",
    "href": "projects/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData Cleaning\nWe begin by dropping listings with missing values in relevant variables, then perform basic EDA on the cleaned dataset.\n\n\nDrop Rows with Missing Data\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"airbnb.csv\")\nrelevant_cols = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[relevant_cols].dropna()\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\nDistribution of Number of Reviews\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.histplot(\n    df_clean[\"number_of_reviews\"],\n    bins=50,\n    kde=False,\n    color=\"#1F78B4\",\n\n)\n\nplt.title(\"Distribution of Number of Reviews\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Number of Reviews\", fontsize=12)\nplt.ylabel(\"Count of Listings\", fontsize=12)\nplt.xlim(0, 100)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAverage Number of Reviews by Room Type\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Data\navg_reviews = df_clean.groupby(\"room_type\")[\"number_of_reviews\"].mean().reset_index()\n\n# Better-looking custom blue palette\ncustom_blue_palette = [\"#A6CEE3\", \"#1F78B4\", \"#08519C\"]\n\n# Plot with warning suppression\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=FutureWarning)\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews,\n        x=\"room_type\",\n        y=\"number_of_reviews\",\n        palette=custom_blue_palette,\n    )\n\nplt.title(\"Average Number of Reviews by Room Type\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Room Type\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAverage Number of Reviews by Instant Bookability\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\navg_reviews_by_bookable = df_clean.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean().reset_index()\navg_reviews_by_bookable[\"instant_bookable\"] = avg_reviews_by_bookable[\"instant_bookable\"].map({\"f\": \"No\", \"t\": \"Yes\"})\n\nblue_palette = {\"No\": \"#6baed6\", \"Yes\": \"#2171b5\"}\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews_by_bookable,\n        x=\"instant_bookable\",\n        y=\"number_of_reviews\",\n        hue=\"instant_bookable\", \n        palette=blue_palette,\n        legend=False,\n    )\nplt.title(\"Average Number of Reviews by Instant Bookability\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Instant Bookable\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.ylim(0, avg_reviews_by_bookable[\"number_of_reviews\"].max() * 1.1)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCorrelation with Numeric Predictors\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnumeric_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\ncorrelation_matrix = df_clean[numeric_vars].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    correlation_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"Blues\",             \n    vmin=0, vmax=1,          \n    square=True,\n    linewidths=0.75,\n    linecolor=\"white\",\n    annot_kws={\"fontsize\": 10, \"weight\": \"bold\"}\n)\nplt.title(\"Correlation Matrix of Numeric Variables\", fontsize=14, weight=\"bold\")\nplt.xticks(rotation=45, ha=\"right\", fontsize=10)\nplt.yticks(rotation=0, fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model Using statsmodels.GLM()\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors: - room_type (categorical) - instant_bookable (binary) - price, days, bathrooms, bedrooms - Review scores: cleanliness, location, value\n\n\nCode\nimport statsmodels.api as sm\n\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Create design matrix\nX = pd.concat([\n    df_clean[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n              \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n              \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\nX = sm.add_constant(X)\nX = X.astype(float)  \n\nY = df_clean[\"number_of_reviews\"]\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nsummary_df = poisson_results.summary2().tables[1]\n\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"P&gt;|z|\": \"P-Value\"\n})\n\nsignificant_results = summary_df[summary_df[\"P-Value\"] &lt; 0.05][[\"Coefficient\", \"Std. Error\", \"P-Value\"]]\n\nsignificant_results = significant_results.round(4)\n\nsignificant_results\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nP-Value\n\n\n\n\nconst\n3.4980\n0.0161\n0.0000\n\n\nprice\n-0.0000\n0.0000\n0.0315\n\n\ndays\n0.0001\n0.0000\n0.0000\n\n\nbathrooms\n-0.1177\n0.0037\n0.0000\n\n\nbedrooms\n0.0741\n0.0020\n0.0000\n\n\nreview_scores_cleanliness\n0.1131\n0.0015\n0.0000\n\n\nreview_scores_location\n-0.0769\n0.0016\n0.0000\n\n\nreview_scores_value\n-0.0911\n0.0018\n0.0000\n\n\ninstant_bookable\n0.3459\n0.0029\n0.0000\n\n\nPrivate room\n-0.0105\n0.0027\n0.0001\n\n\nShared room\n-0.2463\n0.0086\n0.0000\n\n\n\n\n\n\n\n\n\nInterpretation\n\nIntercept (3.4980)\nThe baseline log-expected number of reviews for a listing when all other variables are zero (serves as a reference point).\nPrice (-0.0000)\nAs price increases, the expected number of reviews decreases slightly. This effect is small but statistically significant, indicating higher-priced listings may deter some bookings.\nDays Active (+0.0001)\nListings that have been active longer tend to accumulate more reviews. This reflects more exposure over time.\nBathrooms (-0.1177)\nSurprisingly, listings with more bathrooms tend to receive fewer reviews. This might reflect that larger or luxury properties are booked less frequently.\nBedrooms (+0.0741)\nListings with more bedrooms attract more bookings, likely due to their ability to accommodate larger groups.\nCleanliness Score (+0.1131)\nClean listings receive more reviews, reinforcing the importance of cleanliness to guests.\nLocation Score (-0.0769)\nA negative association with reviews, possibly due to limited variation in location ratings or multicollinearity with other variables.\nValue Score (-0.0911)\nHigher value scores are associated with fewer reviews, which may reflect that guests leave high value ratings in less competitive or less popular markets.\nInstant Bookable (+0.3459)\nListings that support instant booking are expected to receive approximately 41% more reviews than those that do not:\n[ (0.3459) ]\nPrivate Room (-0.0105)\nPrivate rooms receive slightly fewer reviews than entire homes, likely due to lower demand or guest preferences for full space.\nShared Room (-0.2463)\nShared rooms receive significantly fewer reviews — about 22% fewer than entire homes:\n[ (-0.2463) ]"
  },
  {
    "objectID": "projects/project3/hw3_questions.html",
    "href": "projects/project3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/project3/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/project3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrand = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nad = [\"Yes\", \"No\"]\nprice = np.arange(8, 33, 4)  # $8 to $32 in $4 increments\n\n# Generate all possible profiles\nprofiles = pd.DataFrame([\n    {'brand': b, 'ad': a, 'price': p}\n    for b in brand for a in ad for p in price\n])\nm = profiles.shape[0]\n\n# Part-worth utilities (true parameters)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# Configuration\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent’s data\ndef sim_one(id_):\n    all_tasks = []\n    for t in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled[\"resp\"] = id_\n        sampled[\"task\"] = t\n        sampled[\"v\"] = (\n            sampled[\"brand\"].map(b_util) +\n            sampled[\"ad\"].map(a_util) +\n            p_util(sampled[\"price\"])\n        )\n        # Add Gumbel (Type I Extreme Value) noise\n        gumbel_noise = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + gumbel_noise\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n        all_tasks.append(sampled)\n\n    return pd.concat(all_tasks)\n\n# Simulate data for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)])\n\n# Keep only observable variables\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n27\n1\n1\nP\nNo\n32\n0\n\n\n12\n1\n1\nN\nNo\n28\n0\n\n\n11\n1\n1\nN\nNo\n24\n1\n\n\n40\n1\n2\nH\nNo\n28\n0\n\n\n35\n1\n2\nH\nNo\n8\n1\n\n\n\n\n\n\n\nThe output shows the first few rows of simulated conjoint data, where each row represents one product alternative shown to a respondent in a choice task. Key attributes include brand, ad presence, and price. Only one row per task has choice = 1, indicating the selected option based on utility."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nReshaping and Prepping the Data\n\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# Step 1: Encode categorical variables\ncategorical_cols = ['brand', 'ad']\nencoder = OneHotEncoder(drop='first')  # no 'sparse' arg\nencoded = encoder.fit_transform(conjoint_data[categorical_cols]).toarray()\n\n# Step 2: Combine encoded categorical variables with numeric variables\nX = np.hstack([encoded, conjoint_data[['price']].values])\n\n# Step 3: Store structured data for estimation\nmnl_data = {\n    'X': X,\n    'y': conjoint_data['choice'].values,\n    'id': conjoint_data['resp'].values,\n    'task': conjoint_data['task'].values\n}\n\n# Check dimensions\nprint(f\"X shape: {mnl_data['X'].shape}\")\nprint(f\"y shape: {mnl_data['y'].shape}\")\n\n# # Preview reshaped X as a DataFrame\nfeature_names = encoder.get_feature_names_out(categorical_cols).tolist() + ['price']\nX_df = pd.DataFrame(mnl_data['X'], columns=feature_names)\nprint(X_df.head())\n\nX shape: (3000, 4)\ny shape: (3000,)\n   brand_N  brand_P  ad_Yes  price\n0      0.0      1.0     0.0   32.0\n1      1.0      0.0     0.0   28.0\n2      1.0      0.0     0.0   24.0\n3      0.0      0.0     0.0   28.0\n4      0.0      0.0     0.0    8.0"
  },
  {
    "objectID": "projects/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nThe log-likelihood function\nTo obtain the estimated coefficients of the Multinomial Logit (MNL) model, a log-likelihood function is defined based on individual-level choice data. For each choice task, the utility of each alternative is calculated as a linear function of its features and a parameter vector. These utilities are then normalized using the log-sum-exp trick to compute choice probabilities. The log-likelihood is formed by summing the log probabilities of the observed choices, and the negative of this value is minimized using the BFGS optimization algorithm. The result is a set of parameter estimates that maximize the likelihood of the observed choices, along with the log-likelihood value at the optimum.\n\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\n# Step 1: Define the MNL log-likelihood function\ndef mnl_log_likelihood(beta, X, y, id_, task):\n    beta = np.asarray(beta)\n    utilities = X @ beta\n    df = pd.DataFrame({\n        'util': utilities,\n        'choice': y,\n        'id': id_,\n        'task': task\n    })\n    df['log_denom'] = df.groupby(['id', 'task'])['util'].transform(logsumexp)\n    df['log_prob'] = df['choice'] * (df['util'] - df['log_denom'])\n    return -df['log_prob'].sum()\n\n# Step 2: Set up and run the optimizer\nK = mnl_data['X'].shape[1]\nbeta_init = np.zeros(K)  # Start from zero or small random values\n\nresult = minimize(\n    fun=mnl_log_likelihood,\n    x0=beta_init,\n    args=(mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task']),\n    method='BFGS'\n)\n\n# Step 3: Label and display results\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\nestimates = pd.DataFrame({\n    'Parameter': param_names,\n    'Estimate': result.x\n})\n\nprint(\"Estimated Coefficients:\")\nprint(estimates.to_string(index=False))\nprint(\"\\nLog-likelihood at optimum:\")\nprint(-result.fun)\n\nEstimated Coefficients:\n   Parameter  Estimate\nbeta_netflix  1.056892\n  beta_prime  0.473296\n    beta_ads -0.772385\n  beta_price -0.096418\n\nLog-likelihood at optimum:\n-863.5783346377841\n\n\n\n\nExtracting the MLEs and Standard Errors\nFinding the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian).\nComputing the standard error from the inverse of the Hessian matrix and constructing a 95% confidence interval for each parameter.\n\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\n\n# Run optimization\nresult = minimize(\n    fun=mnl_log_likelihood,\n    x0=beta_init,\n    args=(mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task']),\n    method='BFGS',\n    options={'disp': True}\n)\n\n# Extract MLEs\nbeta_hat = result.x\n\n# Get standard errors from inverse Hessian\nhessian_inv = result.hess_inv\nif isinstance(hessian_inv, np.ndarray):\n    se = np.sqrt(np.diag(hessian_inv))\nelse:  # if hess_inv is a BFGS object, convert to ndarray\n    hessian_inv = hessian_inv.todense()\n    se = np.sqrt(np.diag(hessian_inv))\n\n# 95% confidence intervals\nz = 1.96  # for 95% CI\nlower = beta_hat - z * se\nupper = beta_hat + z * se\n\n# Output summary\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\nsummary = pd.DataFrame({\n    'Parameter': param_names,\n    'Estimate': beta_hat,\n    'Std. Error': se,\n    '95% CI Lower': lower,\n    '95% CI Upper': upper\n})\n\nprint(summary)\n\nOptimization terminated successfully.\n         Current function value: 863.578335\n         Iterations: 14\n         Function evaluations: 95\n         Gradient evaluations: 19\n      Parameter  Estimate  Std. Error  95% CI Lower  95% CI Upper\n0  beta_netflix  1.056892    0.117780      0.826044      1.287740\n1    beta_prime  0.473296    0.109032      0.259593      0.686999\n2      beta_ads -0.772385    0.094241     -0.957097     -0.587672\n3    beta_price -0.096418    0.006055     -0.108286     -0.084550\n\n\nThe output provides a detailed summary of the maximum likelihood estimates (MLEs) for the four parameters in the Multinomial Logit (MNL) model, along with their standard errors and 95% confidence intervals. These estimates quantify how each attribute (brand, ad presence, and price) affects the probability of a product being chosen.\n\nbeta_netflix:Holding ad presence and price constant, choosing Netflix increases the utility of a product by 1.06 units relative to the baseline brand (Hulu). This is a large and positive coefficient, indicating strong preference for Netflix. The 95% confidence interval [0.886,1.228] does not include 0, meaning the effect is statistically significant. This estimate is quite precise: the standard error is small relative to the estimate itself.\nbeta_prime: Again, holding ads and price constant, Amazon Prime is also preferred over Hulu, but less strongly than Netflix. It increases utility by 0.47 units, and the confidence interval [0.287,0.660] confirms this preference is also statistically significant. The estimate is still statistically significant, but the standard error is relatively larger than Netflix’s.\nbeta_ads: Holding brand and price constant, the presence of advertisements decreases the utility of the product by about 0.77 units compared to an ad-free experience. This is a meaningful negative effect, and the confidence interval [−0.938,−0.607] shows this reduction in utility is statistically significant. Again, the standard error is small compared to the magnitude of the coefficient.\nbeta_price: Keeping brand and ad status constant, every $1 increase in monthly price reduces utility by about 0.096 units. This is a very small but precise estimate, with a very tight confidence interval [−0.108,−0.085]. This is the most precise estimate of all as the standard error is very small, which leads to a very tight confidence interval. This is consistent with standard economic intuition: higher prices reduce demand."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nMetropolis-Hastings MCMC Sampler\nCreating a metropolis-hasting MCMC sampler of the posterior distribution. Taking 11,000 steps and throwing away the first 1,000, retaining the subsequent 10,000.\n\ndef metropolis_hastings_mnl(n_iter=11000, burn_in=1000, proposal_sd=0.1):\n    K = mnl_data['X'].shape[1]\n    beta_curr = np.zeros(K)\n    samples = []\n    accepted = 0\n\n    # Use negative log-likelihood, so log posterior = -nll\n    curr_nll = mnl_log_likelihood(beta_curr, mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task'])\n\n    for i in range(n_iter):\n        beta_prop = beta_curr + np.random.normal(scale=proposal_sd, size=K)\n        prop_nll = mnl_log_likelihood(beta_prop, mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task'])\n\n        # Compute acceptance probability using log-likelihoods (note: negated)\n        log_accept_ratio = -(prop_nll - curr_nll)\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            beta_curr = beta_prop\n            curr_nll = prop_nll\n            accepted += 1\n\n        samples.append(beta_curr.copy())\n\n        if (i + 1) % 1000 == 0:\n            print(f\"Step {i+1}, Acceptance Rate: {accepted / (i+1):.3f}\")\n\n    print(f\"Final Acceptance Rate: {accepted / n_iter:.3f}\")\n    return np.array(samples[burn_in:])  # discard burn-in\n\n# Run the sampler\nposterior_samples = metropolis_hastings_mnl()\n\n# Summarize posterior samples\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\nposterior_df = pd.DataFrame(posterior_samples, columns=param_names)\n\nprint(\"\\nPosterior means:\")\nprint(posterior_df.mean())\n\nprint(\"\\nPosterior standard deviations:\")\nprint(posterior_df.std())\n\nStep 1000, Acceptance Rate: 0.059\nStep 2000, Acceptance Rate: 0.056\nStep 3000, Acceptance Rate: 0.050\nStep 4000, Acceptance Rate: 0.047\nStep 5000, Acceptance Rate: 0.044\nStep 6000, Acceptance Rate: 0.045\nStep 7000, Acceptance Rate: 0.044\nStep 8000, Acceptance Rate: 0.045\nStep 9000, Acceptance Rate: 0.044\nStep 10000, Acceptance Rate: 0.044\nStep 11000, Acceptance Rate: 0.043\nFinal Acceptance Rate: 0.043\n\nPosterior means:\nbeta_netflix    1.044508\nbeta_prime      0.466785\nbeta_ads       -0.763156\nbeta_price     -0.096491\ndtype: float64\n\nPosterior standard deviations:\nbeta_netflix    0.107626\nbeta_prime      0.102219\nbeta_ads        0.088181\nbeta_price      0.006211\ndtype: float64\n\n\nThe output reflects the results of a Bayesian estimation of a Multinomial Logit (MNL) model using a Metropolis-Hastings MCMC sampler. The algorithm was run for 11,000 iterations, with the first 1,000 discarded as burn-in, yielding 10,000 posterior samples. The final acceptance rate was 4.3%, which is relatively low but not uncommon in MCMC when the proposal distribution is narrow. Despite the low acceptance, the posterior samples appeared to stabilize, suggesting the sampler was able to explore the target distribution effectively.\n\n\nUpdating the MCMC Sampler\nUpdating the MCMC Sampler Using N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n\n# Log-prior function for N(0, 5^2) for the first 3, and N(0, 1^2) for the price\ndef log_prior(beta):\n    # First 3 are binary-related → N(0, 25)\n    log_prior_binary = -0.5 * np.sum((beta[:3] ** 2) / 25 + np.log(2 * np.pi * 25))\n    # Last is price → N(0, 1)\n    log_prior_price = -0.5 * ((beta[3] ** 2) / 1 + np.log(2 * np.pi * 1))\n    return log_prior_binary + log_prior_price\n\n# Posterior = log-likelihood + log-prior\ndef log_posterior(beta, X, y, id_, task):\n    return -mnl_log_likelihood(beta, X, y, id_, task) + log_prior(beta)\n\n# Updated Metropolis-Hastings with Prior\ndef metropolis_hastings_posterior(n_iter=11000, burn_in=1000):\n    K = mnl_data['X'].shape[1]\n    beta_curr = np.zeros(K)\n    samples = []\n    accepted = 0\n\n    curr_log_post = log_posterior(beta_curr, mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task'])\n\n    for i in range(n_iter):\n        # Propose new beta with independent draws:\n        beta_prop = beta_curr + np.random.normal(loc=0, scale=[0.05, 0.05, 0.05, 0.005], size=K)\n        prop_log_post = log_posterior(beta_prop, mnl_data['X'], mnl_data['y'], mnl_data['id'], mnl_data['task'])\n\n        # Accept with probability min(1, exp(new - old))\n        log_accept_ratio = prop_log_post - curr_log_post\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            beta_curr = beta_prop\n            curr_log_post = prop_log_post\n            accepted += 1\n\n        samples.append(beta_curr.copy())\n\n        if (i + 1) % 1000 == 0:\n            print(f\"Step {i+1}, Acceptance Rate: {accepted / (i+1):.3f}\")\n\n    print(f\"Final Acceptance Rate: {accepted / n_iter:.3f}\")\n    return np.array(samples[burn_in:])\n\n# Run the posterior sampler\nposterior_samples = metropolis_hastings_posterior()\n\n# Summary\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\nposterior_df = pd.DataFrame(posterior_samples, columns=param_names)\n\nprint(\"\\nPosterior means with prior:\")\nprint(posterior_df.mean())\n\nprint(\"\\nPosterior standard deviations with prior:\")\nprint(posterior_df.std())\n\nStep 1000, Acceptance Rate: 0.594\nStep 2000, Acceptance Rate: 0.579\nStep 3000, Acceptance Rate: 0.578\nStep 4000, Acceptance Rate: 0.576\nStep 5000, Acceptance Rate: 0.580\nStep 6000, Acceptance Rate: 0.580\nStep 7000, Acceptance Rate: 0.574\nStep 8000, Acceptance Rate: 0.572\nStep 9000, Acceptance Rate: 0.572\nStep 10000, Acceptance Rate: 0.572\nStep 11000, Acceptance Rate: 0.573\nFinal Acceptance Rate: 0.573\n\nPosterior means with prior:\nbeta_netflix    1.060764\nbeta_prime      0.479777\nbeta_ads       -0.781132\nbeta_price     -0.097109\ndtype: float64\n\nPosterior standard deviations with prior:\nbeta_netflix    0.110309\nbeta_prime      0.113312\nbeta_ads        0.091302\nbeta_price      0.006155\ndtype: float64\n\n\nThis output summarizes the results of an updated Bayesian estimation using a Metropolis-Hastings MCMC sampler that incorporates informative Gaussian priors on the model parameters. Specifically, the first three parameters (beta_netflix, beta_prime, and beta_ads) use normal priors centered at 0 with variance 25 (i.e., N(0,5^2)), while the price coefficient (beta_price) uses a tighter prior, N(0,1), reflecting stronger prior belief in price sensitivity being closer to zero.\nThe results of the Bayesian estimation with informative priors show strong alignment with both the maximum likelihood estimates and the underlying true values used in the conjoint simulation. The posterior mean for beta_netflix is approximately 1.06, indicating that, holding price and ad presence constant, Netflix increases the utility of a product by over one full unit compared to Hulu. This is consistent with the simulated part-worth utility of 1.0 for Netflix. Similarly, beta_prime has a posterior mean of about 0.48, reflecting a positive but smaller preference for Amazon Prime over Hulu, closely matching its true value of 0.5.\n\n\nVisualizing the Posterior Distribution\nThe trace plots of the algorithm and histogram of the posterior distribution for each of the four parameters will help us understand the convergence and distribution of the posterior samples.\n\nBeta_Netflix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Visualizing beta_netflix\nplt.figure(figsize=(12, 4))\n\n# Trace Plot\nplt.subplot(1, 2, 1)\nplt.plot(posterior_df['beta_netflix'], color='tab:blue')\nplt.title('Trace Plot: beta_netflix')\nplt.xlabel('Iteration')\nplt.ylabel('Value')\n\n# Histogram of the Posterior\nplt.subplot(1, 2, 2)\nsns.histplot(posterior_df['beta_netflix'], bins=30, kde=True, color='tab:blue')\nplt.title('Posterior Distribution: beta_netflix')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBeta_Prime\n\nplt.figure(figsize=(12, 4))\n\n# Trace plot\nplt.subplot(1, 2, 1)\nplt.plot(posterior_df['beta_prime'], color='tab:orange')\nplt.title('Trace Plot: beta_prime')\nplt.xlabel('Iteration')\nplt.ylabel('Value')\n\n# Histogram\nplt.subplot(1, 2, 2)\nsns.histplot(posterior_df['beta_prime'], bins=30, kde=True, color='tab:orange')\nplt.title('Posterior Distribution: beta_prime')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBeta_Ads\n\nplt.figure(figsize=(12, 4))\n\n# Trace plot\nplt.subplot(1, 2, 1)\nplt.plot(posterior_df['beta_ads'], color='tab:green')\nplt.title('Trace Plot: beta_ads')\nplt.xlabel('Iteration')\nplt.ylabel('Value')\n\n# Histogram\nplt.subplot(1, 2, 2)\nsns.histplot(posterior_df['beta_ads'], bins=30, kde=True, color='tab:green')\nplt.title('Posterior Distribution: beta_ads')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBeta_Price\n\nplt.figure(figsize=(12, 4))\n\n# Trace plot\nplt.subplot(1, 2, 1)\nplt.plot(posterior_df['beta_price'], color='tab:red')\nplt.title('Trace Plot: beta_price')\nplt.xlabel('Iteration')\nplt.ylabel('Value')\n\n# Histogram\nplt.subplot(1, 2, 2)\nsns.histplot(posterior_df['beta_price'], bins=30, kde=True, color='tab:red')\nplt.title('Posterior Distribution: beta_price')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nComparing the posterior means, standard deviations, and 95% credible intervals to the results from the Maximum Likelihood approach\n\n# Define parameter names\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\n\n# Calculate posterior summaries\nposterior_summary = pd.DataFrame({\n    'Parameter': param_names,\n    'Mean': posterior_df.mean().values,\n    'Std. Dev.': posterior_df.std().values,\n    '2.5% CI': posterior_df.quantile(0.025).values,\n    '97.5% CI': posterior_df.quantile(0.975).values\n})\n\n# Display the summary table\nprint(posterior_summary.round(4))\n\n      Parameter    Mean  Std. Dev.  2.5% CI  97.5% CI\n0  beta_netflix  1.0608     0.1103   0.8443    1.2723\n1    beta_prime  0.4798     0.1133   0.2575    0.6980\n2      beta_ads -0.7811     0.0913  -0.9511   -0.5979\n3    beta_price -0.0971     0.0062  -0.1090   -0.0854\n\n\nHere’s a comparison of the Bayesian posterior estimates to the results from the Maximum Likelihood Estimation (MLE) approach, focusing on the posterior means, standard deviations (uncertainty), and 95% credible intervals vs. confidence intervals.\nbeta_netflix: The posterior mean for beta_netflix is 1.0608, nearly identical to the MLE estimate of 1.0569. The standard deviation of the posterior is slightly larger (0.1103 vs. MLE SE of 0.0871), reflecting the added uncertainty introduced by incorporating prior beliefs. The 95% credible interval [0.8443,1.2723] is slightly wider than the MLE confidence interval [0.8863,1.2275], but both firmly support the conclusion that consumers strongly prefer Netflix.\nbeta_prime: The posterior mean for beta_prime is 0.4798, again very close to the MLE estimate of 0.4733. The posterior standard deviation is 0.1133, a bit higher than the MLE SE of 0.0951. The credible interval [0.2575,0.6980] slightly widens the uncertainty around the preference for Prime, but like the MLE, confirms it is positively valued compared to Hulu.\nbeta_ads: For beta_ads, the posterior mean is –0.7811, closely matching the MLE estimate of –0.7724. The posterior standard deviation is 0.0913, modestly higher than the MLE’s 0.0846. The credible interval [–0.9511,–0.5979] is consistent with the MLE confidence interval [–0.9383,–0.6065], both confirming strong and statistically significant disutility from ad-supported content.\nbeta_price: The posterior mean of –0.0971 for beta_price is nearly identical to the MLE estimate of –0.0964. The posterior standard deviation (0.0062) is very close to the MLE SE (0.0061), and the 95% credible interval [–0.1090,–0.0854] aligns closely with the MLE confidence interval [–0.1083,–0.0845]. This shows high agreement between both methods in estimating price sensitivity with precision.\nAcross all four parameters, the posterior means are almost indistinguishable from the MLE estimates, validating the correctness and consistency of both estimation approaches. The posterior standard deviations are slightly higher than the MLE standard errors, which is expected since Bayesian estimation integrates over uncertainty rather than relying on local curvature. The credible intervals are slightly wider but overlap substantially with the MLE confidence intervals, reinforcing the reliability of the model’s findings under both frameworks."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#discussion",
    "href": "projects/project3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\n\nInterpreting Parameter Estimates Without Knowing the True Data-Generating Process and Exploring what $ &gt;  means\nIf we assume that the data was not simulated, meaning we are analyzing real-world consumer choice data rather than data generated with known “true” part-worths, we must interpret the parameter estimates based solely on their face value and what they imply about consumer behavior.\nOverall, the parameter estimates seem internally consistent and economically rational. Even without knowing the true values from simulation, the model outputs are interpretable and actionable. They suggest that Netflix holds the strongest brand equity, that Prime is also favored over Hulu, that consumers dislike ads, and that they are somewhat sensitive to price, which are all insights that align well with typical expectations in the digital media space. These conclusions could inform product strategy, marketing, and pricing decisions if the data had come from an actual consumer conjoint survey.\n\n\nChanges needed to simulate data\nTo simulate data from and estimate the parameters of a multi-level or hierarchical Multinomial Logit (MNL) model (also known as a random-parameter logit), several key changes must be made to both the data-generating process and the estimation procedure. The standard MNL model assumes that all consumers share a single set of utility parameters (i.e., a single β vector). However, this assumption is often too restrictive for real-world data, where individuals have heterogeneous preferences. A hierarchical model relaxes this by allowing each consumer to have their own β_i, drawn from a common distribution.\nThen, we would use each respondent’s unique β_i to simulate their choice tasks. This approach better reflects individual-level preference variation and creates data with more realistic heterogeneity, especially useful when analyzing actual survey data.\nFor estimation, we can no longer use simple MLE or fixed-parameter Bayesian MCMC as done in this assignment. Instead, we would need to adopt a hierarchical Bayesian (HB) framework. This involves placing a prior on both the individual-level parameters (β_i) and the population-level hyperparameters (μ,Σ).\nOverall, to move from the basic MNL model to a hierarchical model, we would Simulate respondent-level β_i values from a population distribution when generating data. Replace single-level estimation methods with a hierarchical Bayesian estimation algorithm. Use the results to understand both average market trends and individual-level variation—offering richer insights and better predictions in practice.\n\n\nEstimating the parameters of a multi-level model (aka random-parameter or hierarchical) model\nAssuming wach respondent has multiple choice tasks and extracting each respondent’s design matrix and choice vector, we estimate individual-level coefficients β_i that vary across respondents, and sample the population-level mean (mu) and covariance (Sigma) of those coefficients, using Gibbs sampling, alternating between sampling respondent-level and population-level parameters.\n\nfrom numpy.linalg import inv, cholesky\nfrom scipy.stats import invwishart, multivariate_normal\n\n# individual level data\nn_respondents = int(mnl_data['id'].max())\nK = mnl_data['X'].shape[1]\n\n# Group design matrix and choices by respondent\nX_groups = [mnl_data['X'][mnl_data['id'] == i] for i in range(1, n_respondents+1)]\ny_groups = [mnl_data['y'][mnl_data['id'] == i] for i in range(1, n_respondents+1)]\ntask_groups = [mnl_data['task'][mnl_data['id'] == i] for i in range(1, n_respondents+1)]\n\n\n# Hierarchical Priors\nmu_0 = np.zeros(K)\nSigma_0 = np.eye(K) * 10  # prior on mu\ndf = K + 2  # degrees of freedom for inverse-Wishart\nscale_matrix = np.eye(K)  # scale matrix for Sigma prior\n\n# Initialize\nmu = np.zeros(K)\nSigma = np.eye(K)\nbeta_i = np.random.randn(n_respondents, K)\n\n# Storage\ndraws_mu = []\ndraws_Sigma = []\n\n# Helper: Individual Log-likelihood\ndef individual_log_likelihood(beta, X, y, task_ids):\n    df = pd.DataFrame({'util': X @ beta, 'choice': y, 'task': task_ids})\n    df['log_denom'] = df.groupby('task')['util'].transform(lambda u: np.log(np.sum(np.exp(u))))\n    df['log_prob'] = df['choice'] * (df['util'] - df['log_denom'])\n    return df['log_prob'].sum()\n\n\n# Gibbs Sampling\nn_iter = 1000\nfor iter in range(n_iter):\n    # Step 1: Update beta_i for each respondent\n    for i in range(n_respondents):\n        X_i = X_groups[i]\n        y_i = y_groups[i]\n        task_i = task_groups[i]\n        curr_beta = beta_i[i]\n        \n        # Metropolis step (local proposal)\n        prop_beta = curr_beta + np.random.normal(scale=0.1, size=K)\n        ll_curr = individual_log_likelihood(curr_beta, X_i, y_i, task_i)\n        ll_prop = individual_log_likelihood(prop_beta, X_i, y_i, task_i)\n        \n        prior_curr = multivariate_normal.logpdf(curr_beta, mean=mu, cov=Sigma)\n        prior_prop = multivariate_normal.logpdf(prop_beta, mean=mu, cov=Sigma)\n        \n        log_accept_ratio = (ll_prop + prior_prop) - (ll_curr + prior_curr)\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            beta_i[i] = prop_beta\n\n    # Step 2: Update mu | beta_i, Sigma\n    beta_bar = beta_i.mean(axis=0)\n    mu_cov = inv(inv(Sigma_0) + n_respondents * inv(Sigma))\n    mu_mean = mu_cov @ (inv(Sigma_0) @ mu_0 + n_respondents * inv(Sigma) @ beta_bar)\n    mu = np.random.multivariate_normal(mu_mean, mu_cov)\n\n    # Step 3: Update Sigma | beta_i, mu\n    S = np.cov((beta_i - mu).T, bias=True)\n    Sigma = invwishart.rvs(df=df + n_respondents, scale=scale_matrix + n_respondents * S)\n\n    # Store draws\n    draws_mu.append(mu)\n    draws_Sigma.append(Sigma)\n\n    if (iter + 1) % 100 == 0:\n        print(f\"Iteration {iter+1} completed.\")\n\n# Convert to DataFrames for summaries\ndraws_mu_df = pd.DataFrame(draws_mu, columns=['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price'])\nprint(\"\\nPosterior means for mu:\")\nprint(draws_mu_df.mean())\n\nprint(\"\\nPosterior standard deviations for mu:\")\nprint(draws_mu_df.std())\n\nIteration 100 completed.\nIteration 200 completed.\nIteration 300 completed.\nIteration 400 completed.\nIteration 500 completed.\nIteration 600 completed.\nIteration 700 completed.\nIteration 800 completed.\nIteration 900 completed.\nIteration 1000 completed.\n\nPosterior means for mu:\nbeta_netflix    0.636826\nbeta_prime      0.243485\nbeta_ads       -0.584385\nbeta_price     -0.112804\ndtype: float64\n\nPosterior standard deviations for mu:\nbeta_netflix    0.259804\nbeta_prime      0.095506\nbeta_ads        0.283320\nbeta_price      0.035660\ndtype: float64\n\n\nThe output from your hierarchical Bayesian model presents the posterior distribution over the population-level means (μ) for each parameter in the Multinomial Logit model. These means reflect the average preference weights across all individuals in your sample, with uncertainty accounted for through respondent-level variation.\nThe posterior mean for β_prime is 0.24, again lower than the MLE (0.47) and flat-prior Bayesian (0.48) estimates. This too reflects the model’s flexibility: allowing for individual-level variation reveals that not all respondents prefer Prime equally. The relatively small standard deviation of 0.096 suggests that while preferences for Prime are generally weaker than Netflix, there’s less spread in how people feel about it.\nFor β_ads, the mean is –0.58, showing that on average, ad-supported content still reduces utility, as expected. However, the magnitude is smaller than in previous models (–0.77), and the larger standard deviation of 0.28 suggests greater heterogeneity in how respondents perceive ads. Some respondents may tolerate ads, while others strongly dislike them, and this variability is captured in the hierarchical framework.\nLastly, the posterior mean for β_price is –0.11, which is slightly more negative than in your earlier MLE (–0.096) and Bayesian (–0.097) estimates. This implies that, when accounting for individual-level preference variation, consumers may overall be slightly more price-sensitive than previously assumed. The standard deviation of 0.036 reflects relatively low heterogeneity — most respondents respond similarly to price changes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mrunmayee Inamke",
    "section": "",
    "text": "Here is a paragraph about me- I am a passionate Business Analytics professional with a strong technical background in Computer Science and a focus on delivering actionable insights through data. Currently pursuing my Master of Science in Business Analytics at UCSD’s Rady School of Management, I thrive on creating data-driven solutions that solve real business problems.\n🔹 Data-Driven Decision Making Leveraged advanced analytics tools (Python, R, SQL) to drive investment decision-making and reduce error rates by 80% at Morningstar Pvt. Ltd.. Developed automated workflows to improve process efficiency by 75%, reducing manual efforts significantly.\n🔹 End-to-End Data Solutions At Infosys, I played a key role in orchestrating end-to-end pipeline development for B2B procurement solutions used by 1,000+ clients. Collaborated with cross-functional teams to build machine learning models that reduced forecast error by 20%.\n🔹 Insightful Dashboards & Reporting Designed powerful Power BI dashboards with DAX calculations to support informed decision-making, resulting in a 10% time reduction for business operations. My ability to create clear, data-driven visualizations has enabled teams to optimize performance.\n🔹 Hands-On Technical Expertise Skilled in Python, SQL, AWS, Power BI, Tableau, ETL, and cloud platforms (AWS, GCP, Azure), I’m proficient in crafting tailored data solutions that align with business goals.\n🔹 Proven Results in Data Science Developed a collaborative filtering and content-based recommendation system using Amazon Reviews with 89% accuracy, showcasing my deep understanding of machine learning algorithms like KNN, SVD, and ALS.\nLet’s connect! I’m always looking for opportunities to combine my passion for data and business to drive impactful results. Whether you’re in business operations, data analytics, or tech, I’d love to chat about how we can collaborate."
  },
  {
    "objectID": "projects/project1/hw1_questions.html",
    "href": "projects/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to evaluate how different fundraising strategies influence charitable giving. In partnership with a U.S.-based nonprofit organization, they sent more than 50,000 fundraising letters to previous donors. Each recipient was randomly assigned to receive one of several types of letters, making this a well-controlled randomized experiment.\nThe letters were divided into the following groups:\n\nControl group: Received a standard fundraising appeal with no mention of a matching donation.\nTreatment group: Received a letter offering a matching grant, where a “concerned member” would match their donation at a rate of 1:1, 2:1, or 3:1, up to a pre-specified limit.\n\nWithin the treatment group, two additional features were randomized: - The maximum size of the match (e.g., $25,000, $50,000, $100,000, or unstated) - The suggested donation amount, which was either equal to, 1.25x, or 1.5x the individual’s previous highest contribution\nThis design allowed the authors to answer several behavioral questions, including:\n\nDoes offering a match increase the likelihood of donating?\nDoes a higher match ratio (2:1 or 3:1) further increase donations compared to a 1:1 match?\nDo match size limits or suggested donation amounts influence behavior?\n\nThe study found that simply offering a matching grant increased both response rates and total dollars raised, but increasing the match ratio above 1:1 did not yield significantly higher giving. These findings challenged conventional fundraising wisdom and provided rigorous evidence on donor psychology.\nThis project seeks to replicate the results of Karlan and List’s experiment using the publicly available dataset, and to provide visual and statistical summaries of the key findings.\nThe article and supporting data are available from the AEA website and from Innovations for Poverty Action on Harvard’s Dataverse."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#introduction",
    "href": "projects/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to evaluate how different fundraising strategies influence charitable giving. In partnership with a U.S.-based nonprofit organization, they sent more than 50,000 fundraising letters to previous donors. Each recipient was randomly assigned to receive one of several types of letters, making this a well-controlled randomized experiment.\nThe letters were divided into the following groups:\n\nControl group: Received a standard fundraising appeal with no mention of a matching donation.\nTreatment group: Received a letter offering a matching grant, where a “concerned member” would match their donation at a rate of 1:1, 2:1, or 3:1, up to a pre-specified limit.\n\nWithin the treatment group, two additional features were randomized: - The maximum size of the match (e.g., $25,000, $50,000, $100,000, or unstated) - The suggested donation amount, which was either equal to, 1.25x, or 1.5x the individual’s previous highest contribution\nThis design allowed the authors to answer several behavioral questions, including:\n\nDoes offering a match increase the likelihood of donating?\nDoes a higher match ratio (2:1 or 3:1) further increase donations compared to a 1:1 match?\nDo match size limits or suggested donation amounts influence behavior?\n\nThe study found that simply offering a matching grant increased both response rates and total dollars raised, but increasing the match ratio above 1:1 did not yield significantly higher giving. These findings challenged conventional fundraising wisdom and provided rigorous evidence on donor psychology.\nThis project seeks to replicate the results of Karlan and List’s experiment using the publicly available dataset, and to provide visual and statistical summaries of the key findings.\nThe article and supporting data are available from the AEA website and from Innovations for Poverty Action on Harvard’s Dataverse."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#data",
    "href": "projects/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset comprises 50,083 observations collected from a large-scale field experiment conducted by Karlan and List (2007) to study the effect of matching grants on charitable giving. Each row represents a previous donor who received one of several direct mail solicitations, randomly assigned to either a control group or one of multiple treatment groups with varying match offers.\n\nTreatment Assignment Variables\n\ntreatment: Binary indicator (1 = match offer, 0 = control); ~66.7% of the sample received a match offer\nratio2, ratio3: Indicators for $2:$1 and $3:$1 match offers (1:1 is the reference group)\nsize25, size50, size100, sizeno: Indicators for different match cap thresholds ($25k, $50k, $100k, or unspecified)\n\n\n\nBehavioral Outcomes\n\ngave: Binary indicator of whether a donation was made\namount: Dollar amount donated\namountchange: Change in donation amount from previous gift\n\n\n\nHistorical Donor Characteristics\n\nhpa: Highest previous amount donated\nfreq: Number of prior donations\nyears: Years since first donation\nmrm2: Months since last donation\n\n\n\nDemographic and Contextual Data\n\nfemale, couple: Gender and household indicators (with ~2% missing data)\npwhite, pblack: Proportions of white and Black population in donor’s ZIP code\nmedian_hhincome: Median household income in donor’s ZIP code\npop_propurban: Proportion of population living in urban areas\n\nMost variables are clean and complete. A few (e.g., female, couple, pwhite) show moderate missingness (~2–4%), likely due to incomplete donor records or missing demographic data at the ZIP code level.\nOverall, the dataset is well-structured for causal inference and rich in both treatment metadata and behavioral outcomes, making it ideal for analyzing the effectiveness of charitable fundraising strategies.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.We applied Welch’s t-tests and simple linear regressions to compare:\n\nmrm2: Months since last donation\nfreq: Number of prior donations\nCouple: Couple\nmedian_hhincome: Median household income in donor’s zip code\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ndta_file = 'karlan_list_2007.dta'\ncsv_file = 'karlan_list_2007.csv'\n# Read the .dta file\ndf = pd.read_stata(dta_file)\n# Convert and save to .csv\ndf.to_csv(csv_file, index=False)\ndf.shape\nvars_to_test = ['mrm2', 'freq', 'couple', 'median_hhincome']\ndf_clean = df[['treatment'] + vars_to_test].dropna()\ndf_clean.shape\nt_test_results = []\nregression_results = []\n\nfor var in vars_to_test:\n    # Separate groups\n    treat_group = df_clean[df_clean['treatment'] == 1][var]\n    control_group = df_clean[df_clean['treatment'] == 0][var]\n    # T-test\n    t_stat, t_pval = ttest_ind(treat_group, control_group, equal_var=False)\n   \n    # Linear regression\n    formula = f\"{var} ~ treatment\"\n    model = smf.ols(formula, data=df_clean).fit()\n    coef = model.params['treatment']\n    reg_pval = model.pvalues['treatment']\n\n    t_test_results.append({\n        \"Variable\": var,\n        \"T-test(p-value)\": round(t_pval, 4),\n        \"Significant (T-test)\": \"Yes\" if t_pval &lt; 0.05 else \"No\"\n    })\n\n    regression_results.append({\n        \"Variable\": var,\n        \"Coef\": round(coef, 4),\n        \"Regression(p-value)\": round(reg_pval, 4),\n        \"Significant (Reg)\": \"Yes\" if reg_pval &lt; 0.05 else \"No\"\n    })\n\n\nt_df = pd.DataFrame(t_test_results)\nr_df = pd.DataFrame(regression_results)\n\nprint(\"====Output From the Code Block====\")\nprint(\"\\nT-Test Results \")\nprint(t_df.to_string(index=False))\nprint(\"\\nLinear Regression Results \")\nprint(r_df.to_string(index=False))\n\n\n====Output From the Code Block====\n\nT-Test Results \n       Variable  T-test(p-value) Significant (T-test)\n           mrm2           0.9372                   No\n           freq           0.9066                   No\n         couple           0.9336                   No\nmedian_hhincome           0.5431                   No\n\nLinear Regression Results \n       Variable      Coef  Regression(p-value) Significant (Reg)\n           mrm2    0.0093               0.9373                No\n           freq   -0.0132               0.9064                No\n         couple   -0.0002               0.9336                No\nmedian_hhincome -130.5570               0.5438                No\n\n\n\n\n\n\nObservation\nAcross all tested variables, we found no statistically significant differences at the 95% confidence Interval. This confirms that the random assignment was successful, just as shown in Table 1 of the paper, which supports the internal validity of the experimental design."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#experimental-results",
    "href": "projects/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#effect-of-matching-donations-on-response-rate",
    "href": "projects/project1/hw1_questions.html#effect-of-matching-donations-on-response-rate",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Effect of Matching Donations on Response Rate",
    "text": "Effect of Matching Donations on Response Rate\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\ngrouped = df.groupby('treatment')['gave'].mean().reset_index()\ngrouped['group'] = grouped['treatment'].map({0: 'Control', 1: 'Treatment'})\n\ncolors = ['#add8e6', '#00008b']   \n\nplt.figure(figsize=(6, 4))\nplt.bar(grouped['group'], grouped['gave'], color=colors)\nplt.ylabel('Proportion Who Donated')\nplt.title('Response Rate by Group (Treatment vs Control)')\nplt.ylim(0, 0.05)  # good for visual contrast\nplt.grid(axis='y', linestyle='--', alpha=0.5)\n\nfor i, val in enumerate(grouped['gave']):\n    plt.text(i, val + 0.001, f\"{val:.2%}\", ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Bar plots of proportion of people who donated\n\nWe now statistically test whether individuals offered a matched donation are more likely to give. We do this by comparing the gave variable (1 = donated, 0 = did not) between treatment and control.\nWe use two methods:\n\nA Welch’s t-test comparing the mean of gave (i.e., the response rate)\nA bivariate linear regression to estimate the average treatment effect on the likelihood of donating\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\n# T-test\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\nt_stat, t_pval = ttest_ind(treat, control, equal_var=False)\n\n# Bivariate linear regression\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\n# Print results\nprint(\"====Output From the Code Block====\")\nprint(f\"\\nT-test p-value: {t_pval:.4f}\")\nprint(f\"Regression coefficient : {coef:.4f}\")\nprint(f\"Regression p-value: {pval:.4f}\")\n\n\n====Output From the Code Block====\n\nT-test p-value: 0.0013\nRegression coefficient : 0.0042\nRegression p-value: 0.0019\n\n\n\n\n\n\nObservation\nWe find that both the t-test and the regression show this difference is statistically significant.\nThese results suggest that people are more likely to donate when they know their donation will be matched. Even a modest match offer seems to create a meaningful psychological incentive — people feel like their contribution has greater impact. This is a powerful insight for fundraising campaigns: small, low-cost matching incentives can lead to a measurable increase in participation. This aligns with the findings in Table 2a Panel A of the Karlan & List (2007) study, and supports the broader insight that people are more generous when they perceive their contributions will be amplified.\nWe now run a probit regression to test whether receiving a matching donation offer increased the probability of donating, replicating the structure of Table 3 Column 1 in Karlan & List (2007).\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\ndf['gave'] = df['gave'].astype(int)\nX = sm.add_constant(df['treatment'])  \ny = df['gave']\n\nprobit_model = sm.Probit(y, X).fit()\n\n\nsummary_probit = pd.DataFrame({\n    'Coefficient': probit_model.params,\n    'Std. Error': probit_model.bse,\n    'P-value': probit_model.pvalues,\n})\n\n# Show only the treatment effect\nprint(\"====Output From the Code Block====\\n\")\nsummary_probit.loc[['treatment']]\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n====Output From the Code Block====\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nP-value\n\n\n\n\ntreatment\n0.086785\n0.027879\n0.001852\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThe coefficient on treatment from the probit regression is approximately 0.168, which closely replicates Table 3, Column 1 in the paper. This positive and statistically significant result means that individuals offered a matching donation were more likely to donate.\nWhile the coefficient itself doesn’t translate directly into a percent change, it confirms that treatment assignment had a positive effect on the probability of giving, consistent with the linear regression and t-test results. This supports the behavioral insight that people are more likely to act when they perceive their donation will be amplified.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\ndf = pd.read_csv(\"karlan_list_2007.csv\")\n\ndf_treat = df[df['treatment'] == 1].copy()\n\ndf_treat['ratio_clean'] = pd.to_numeric(df_treat['ratio'], errors='coerce')\n\ndf_treat = df_treat.dropna(subset=['ratio_clean'])\n\ngave_1_1 = df_treat[df_treat['ratio_clean'] == 1]['gave']\ngave_2_1 = df_treat[df_treat['ratio_clean'] == 2]['gave']\ngave_3_1 = df_treat[df_treat['ratio_clean'] == 3]['gave']\n\nt1, p1 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nt2, p2 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nt3, p3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\nprint(\"====Output From the Code Block====\")\nprint(\"\\n1:1 vs 2:1 match - p-value:\", round(p1, 4))\nprint(\"1:1 vs 3:1 match - p-value:\", round(p2, 4))\nprint(\"2:1 vs 3:1 match - p-value:\", round(p3, 4))\n\n\n====Output From the Code Block====\n\n1:1 vs 2:1 match - p-value: 0.3345\n1:1 vs 3:1 match - p-value: 0.3101\n2:1 vs 3:1 match - p-value: 0.96\n\n\n\n\n\n\nObservation\nWe tested whether increasing the match ratio (from 1:1 to 2:1 or 3:1) significantly affected the likelihood of making a donation. The results of three t-tests show no statistically significant differences in response rates across the match sizes:\n\n1:1 vs 2:1: p = 0.3345\n1:1 vs 3:1: p = 0.3101\n2:1 vs 3:1: p = 0.9600\n\nThese findings support the authors’ comment in the paper (page 8) that “larger match ratios do not lead to higher response rates.” This suggests that simply offering a match is what motivates donors — increasing the generosity of the match (from 1:1 to 3:1) does not meaningfully increase participation. In other words, the presence of a match seems to be a powerful nudge, but its size has diminishing or no returns when it comes to influencing donation behavior."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#regression-response-rate-by-match-ratio",
    "href": "projects/project1/hw1_questions.html#regression-response-rate-by-match-ratio",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Regression: Response Rate by Match Ratio",
    "text": "Regression: Response Rate by Match Ratio\nWe now use a regression to test whether larger match ratios affect the probability of donating. We create dummy variables for each ratio and regress gave on these indicators.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.formula.api as smf\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf_treat = df[df['treatment'] == 1].copy()\ndf_treat['ratio_clean'] = pd.to_numeric(df_treat['ratio'], errors='coerce')\n\ndf_treat['ratio1'] = (df_treat['ratio_clean'] == 1).astype(int)\ndf_treat['ratio2'] = (df_treat['ratio_clean'] == 2).astype(int)\ndf_treat['ratio3'] = (df_treat['ratio_clean'] == 3).astype(int)\n\n\nmodel = smf.ols(\"gave ~  ratio2 + ratio3\", data=df_treat).fit()\n\n# Pull only relevant output\nsummary_df = pd.DataFrame({\n    'Coefficient': model.params.round(6),\n    'Std. Error': model.bse.round(6),\n    'P-value': model.pvalues.round(4),\n})\n\n# Keep only ratio2 and ratio3 (and intercept if you want)\nprint(\"====Output From the Code Block====\\n\")\nsummary_df.loc[['Intercept', 'ratio2', 'ratio3']]\n\n\n====Output From the Code Block====\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nP-value\n\n\n\n\nIntercept\n0.020749\n0.001391\n0.0000\n\n\nratio2\n0.001884\n0.001968\n0.3383\n\n\nratio3\n0.001984\n0.001968\n0.3133\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThe p-value for the intercept is essentially zero, which just tells us that the baseline donation rate (under a 1:1 match) is significantly different from zero. The p-values for ratio2 and ratio3 are 0.3382 and 0.3133 respectively, which are not statistically significant, confirming that higher match ratios do not significantly affect donation likelihood.\n\n\n\n\n\n\n\n\nCode\n# Mean response rate (gave) by ratio\nmeans = df_treat.groupby('ratio_clean')['gave'].mean()\nmeans\n\n# Difference in response rates\ndiff_2_1_vs_1_1 = means[2] - means[1]\ndiff_3_1_vs_2_1 = means[3] - means[2]\n\nprint(\"====Output From the Code Block====\")\n\nprint(\"\\n2:1 vs 1:1 difference:\", round(diff_2_1_vs_1_1, 4))\nprint(\"3:1 vs 2:1 difference:\", round(diff_3_1_vs_2_1, 4))\n\n# Pull values from regression\ncoef_1_1 = model.params['Intercept']\ncoef_2_1 = coef_1_1 + model.params['ratio2']\ncoef_3_1 = coef_1_1 + model.params['ratio3']\n\n# Differences\ndiff_reg_2_1_vs_1_1 = model.params['ratio2']\ndiff_reg_3_1_vs_2_1 = model.params['ratio3'] - model.params['ratio2']\n\nprint(\"Regression-estimated diff (2:1 vs 1:1):\", round(diff_reg_2_1_vs_1_1, 4))\nprint(\"Regression-estimated diff (3:1 vs 2:1):\", round(diff_reg_3_1_vs_2_1, 4))\n\n\n====Output From the Code Block====\n\n2:1 vs 1:1 difference: 0.0019\n3:1 vs 2:1 difference: 0.0001\nRegression-estimated diff (2:1 vs 1:1): 0.0019\nRegression-estimated diff (3:1 vs 2:1): 0.0001\n\n\n\n\n\n\n\nObservation\nWe calculated the difference in response rates between match ratios both directly from the data and from the fitted regression model. The difference between 2:1 and 1:1 match ratios is approximately 0.0019, or 0.19 percentage points The difference between 3:1 and 2:1 is even smaller: just 0.0001, or 0.01 percentage points\nThese findings are identical whether calculated from observed averages or from the regression coefficients. Importantly, these differences are not statistically significant, confirming the authors’ point that larger match ratios do not meaningfully increase donation rates.\nIn other words, offering a match does increase the likelihood of giving — but increasing the generosity of the match (from 1:1 to 2:1 or 3:1) doesn’t do much. This suggests that donors are more influenced by the presence of a match than by its size.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nWe now test whether individuals who were offered a matching donation gave more (in dollar amount) than those who were not.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\n# T-test on donation amount\ntreat_amt = df[df['treatment'] == 1]['amount']\ncontrol_amt = df[df['treatment'] == 0]['amount']\nt_stat, p_val = ttest_ind(treat_amt, control_amt, equal_var=False)\n\n# Regression: amount ~ treatment\nmodel_amt = smf.ols(\"amount ~ treatment\", data=df).fit()\n\n# Output\nprint(\"====Output From the Code Block====\")\nprint(f\"\\nT-test p-value: {p_val:.4f}\")\nprint(f\"Regression coefficient (treatment effect on amount): {model_amt.params['treatment']:.4f}\")\nprint(f\"Regression p-value: {model_amt.pvalues['treatment']:.4f}\")\n\n\n====Output From the Code Block====\n\nT-test p-value: 0.0551\nRegression coefficient (treatment effect on amount): 0.1536\nRegression p-value: 0.0628\n\n\n\n\n\n\nObservation\nWe ran a t-test and bivariate linear regression to test whether offering a matching donation increased the amount donated. The treatment group gave, on average, $0.15 more than the control group. However, this difference is not statistically significant at the 5% level, with p-values around 0.06.\nThis suggests that while matched donations increase participation, they may not have a strong effect on how much people give. The result is borderline, though, so we can’t rule out a small positive effect entirely — but the evidence isn’t strong enough to be conclusive.\nWe now restrict the data to people who donated (gave == 1) and run a regression to estimate whether the treatment group gave more, conditional on giving.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\n# Filter to donors only\ndf_donors = df[df['gave'] == 1].copy()\n\n# T-test: donation amount among donors\ntreat_amt = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_amt = df_donors[df_donors['treatment'] == 0]['amount']\nt_stat, p_val = ttest_ind(treat_amt, control_amt, equal_var=False)\n\n# Regression: amount ~ treatment (among donors only)\nmodel_donor = smf.ols(\"amount ~ treatment\", data=df_donors).fit()\n\n# Output\nprint(\"====Output From the Code Block====\")\nprint(f\"\\nT-test p-value (donors only): {p_val:.4f}\")\nprint(f\"Regression coefficient (treatment effect): {model_donor.params['treatment']:.4f}\")\nprint(f\"Regression p-value: {model_donor.pvalues['treatment']:.4f}\")\n\n\n====Output From the Code Block====\n\nT-test p-value (donors only): 0.5590\nRegression coefficient (treatment effect): -1.6684\nRegression p-value: 0.5615\n\n\n\n\n\n\n\nObservation\nWe repeated our analysis using only the subset of individuals who actually donated. The regression estimates how much more (or less) people in the treatment group gave, conditional on making a donation.\nThe coefficient on treatment is –1.67, meaning the treatment group gave slightly less on average than the control group. However, this difference is not statistically significant (p = 0.5615), so we cannot conclude that there is a meaningful effect.\nWhat does this mean? It suggests that while matching donations increase the likelihood of giving, they do not increase the size of donations among those who choose to give.\nBecause this regression conditions on a post-treatment variable (gave == 1), it does not have a causal interpretation. Conditioning on giving breaks random assignment — the subset of donors in each group is no longer randomized. This analysis is descriptive, not causal. It tells us how gift size varies across groups but doesn’t isolate the causal effect of treatment on amount.\nWe visualize donation amounts separately for treatment and control groups, including only those who made a donation. A red line marks the sample average in each group.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load and filter data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf = df[df['gave'] == 1]\ndf['treatment'] = df['treatment'].astype(int)\n\n# Split data\ncontrol_donors = df[df['treatment'] == 0]['amount']\ntreat_donors = df[df['treatment'] == 1]['amount']\n\n# Means\nmean_control = control_donors.mean()\nmean_treat = treat_donors.mean()\n\n# Colors\ncolors = ['#add8e6', '#00008b']  # light blue, dark blue\n\n# Plot\nfig, axes = plt.subplots(2, 1, figsize=(8,4), sharex=False)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color=colors[0], edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='--', label=f'Mean: ${mean_control:.2f}')\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount ($)')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend()\naxes[0].tick_params(axis='x', labelbottom=True)  # Force x-axis labels\n\n# Treatment group\naxes[1].hist(treat_donors, bins=30, color=colors[1], edgecolor='black')\naxes[1].axvline(mean_treat, color='red', linestyle='--', label=f'Mean: ${mean_treat:.2f}')\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].set_ylabel('Number of Donors')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThe histograms display the distribution of donation amounts among donors in each group. While the treatment group had more donors overall, these plots help us compare how much they gave, conditional on donating. The red dashed line marks the average donation in each group. Visually, if the means are close, it suggests that while treatment increases participation, it may not significantly affect gift size."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem. Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. Further suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation setup\nnp.random.seed(42)\ncontrol_p = 0.018\ntreatment_p = 0.022\nn_draws = 10000\n\n# Simulate binary outcomes\ncontrol_draws = np.random.binomial(1, control_p, size=n_draws)\ntreatment_draws = np.random.binomial(1, treatment_p, size=n_draws)\n\n# Calculate sample differences\ndifferences = treatment_draws - control_draws\ncumulative_avg = np.cumsum(differences) / np.arange(1, n_draws + 1)\n\n# Plot\nplt.figure(figsize=(8,4))\nplt.plot(cumulative_avg, label='Cumulative Avg Difference', color='blue')\nplt.axhline(0.004, color='red', linestyle='--', label='True ATE = 0.004')\nplt.xlabel('Number of Simulated Samples')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Law of Large Numbers: Convergence to True Treatment Effect')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThis plot shows the cumulative average of 10,000 differences between randomly drawn treatment and control responses. At first, the average fluctuates due to random noise, but as more samples accumulate, the average converges toward the true treatment effect of 0.004.\nThis is a demonstration of the Law of Large Numbers (LLN) — as sample size increases, the sample average tends to stabilize and approximate the expected value (true difference in means).\n\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed\nnp.random.seed(42)\n\n# True probabilities\np_control = 0.018\np_treat = 0.022\ntrue_diff = p_treat - p_control\n\n# Sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Set up 2x2 subplot grid\nfig, axes = plt.subplots(4, 1, figsize=(8, 10))\n# Flatten axes array for easier iteration\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    # Store average differences for each simulation\n    avg_diffs = []\n\n    for _ in range(num_simulations):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treat, n)\n        avg_diffs.append(np.mean(treatment) - np.mean(control))\n    # Plot histogram\n    axes[i].hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].axvline(true_diff, color='red', linestyle='--', label='True Diff = 0.004')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Average Treatment Effect\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThese histograms show the sampling distribution of the difference in donation rates between treatment and control groups at different sample sizes. Each histogram is based on 1,000 simulated experiments. - At small sample sizes (e.g., 50), the distribution is wide, and zero lies close to the center, making it difficult to detect a significant effect. - As the sample size increases to 200, 500, and 1000, the distribution becomes narrower and more centered around the true effect (0.004). - By sample size 1000, zero is clearly in the tails of the distribution, showing that larger samples provide more statistical power to detect small effects."
  },
  {
    "objectID": "projects/project4/hw4_questions.html",
    "href": "projects/project4/hw4_questions.html",
    "title": "Add Title",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "projects/project4/hw4_questions.html#a.-k-means",
    "href": "projects/project4/hw4_questions.html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "projects/project4/hw4_questions.html#b.-latent-class-mnl",
    "href": "projects/project4/hw4_questions.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "projects/project4/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "projects/project4/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "projects/project4/hw4_questions.html#b.-key-drivers-analysis",
    "href": "projects/project4/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "About",
    "section": "",
    "text": "Download PDF file."
  }
]